# PingvasAI SaaS ìµœì¢… êµ¬í˜„ ê°€ì´ë“œ

> InvokeAIë¥¼ ì™„ì „í•œ Multi-Tenant SaaSë¡œ ì „í™˜í•˜ëŠ” ìµœì¢… í†µí•© í•¸ì¦ˆì˜¨ ê°€ì´ë“œ

**ì‘ì„±ì¼**: 2025-11-18
**ëŒ€ìƒ**: ì‹ ì… ê°œë°œìë„ ë”°ë¼í•  ìˆ˜ ìˆëŠ” ë‹¨ê³„ë³„ ê°€ì´ë“œ
**ëª©í‘œ**: React SPA + FastAPI + EKS ê¸°ë°˜ í”„ë¡œë•ì…˜ SaaS êµ¬ì¶•
**ì˜ˆìƒ ê¸°ê°„**: 14ì£¼ (3.5ê°œì›”)
**ì˜ˆìƒ ë¹„ìš©**: $827.15/ì›”

---

## ğŸ“‹ ëª©ì°¨

1. [ì•„í‚¤í…ì²˜ ê°œìš”](#1-ì•„í‚¤í…ì²˜-ê°œìš”)
2. [ì‚¬ì „ ì¤€ë¹„ (Week 0)](#2-ì‚¬ì „-ì¤€ë¹„-week-0)
3. [Phase 1: ê¸°ì´ˆ ì¸í”„ë¼ êµ¬ì¶• (Week 1-2)](#3-phase-1-ê¸°ì´ˆ-ì¸í”„ë¼-êµ¬ì¶•-week-1-2)
4. [Phase 2: EKS í´ëŸ¬ìŠ¤í„° ì„¤ì • (Week 3-4)](#4-phase-2-eks-í´ëŸ¬ìŠ¤í„°-ì„¤ì •-week-3-4)
5. [Phase 3: ì• í”Œë¦¬ì¼€ì´ì…˜ ë§ˆì´ê·¸ë ˆì´ì…˜ (Week 5-6)](#5-phase-3-ì• í”Œë¦¬ì¼€ì´ì…˜-ë§ˆì´ê·¸ë ˆì´ì…˜-week-5-6)
6. [Phase 4: ë©€í‹°í…Œë„Œì‹œ & ì¸ì¦ (Week 7-8)](#6-phase-4-ë©€í‹°í…Œë„Œì‹œ--ì¸ì¦-week-7-8)
7. [Phase 5: ê²°ì œ ì‹œìŠ¤í…œ (Week 9-10)](#7-phase-5-ê²°ì œ-ì‹œìŠ¤í…œ-week-9-10)
8. [Phase 6: ì´ë©”ì¼ ì„œë¹„ìŠ¤ (Week 11)](#8-phase-6-ì´ë©”ì¼-ì„œë¹„ìŠ¤-week-11)
9. [Phase 7: ê²€ìƒ‰ ì„œë¹„ìŠ¤ (Week 12)](#9-phase-7-ê²€ìƒ‰-ì„œë¹„ìŠ¤-week-12)
10. [Phase 8: ëª¨ë‹ˆí„°ë§ & CI/CD (Week 13-14)](#10-phase-8-ëª¨ë‹ˆí„°ë§--cicd-week-13-14)
11. [í…ŒìŠ¤íŠ¸ & ê²€ì¦](#11-í…ŒìŠ¤íŠ¸--ê²€ì¦)
12. [í”„ë¡œë•ì…˜ ë°°í¬](#12-í”„ë¡œë•ì…˜-ë°°í¬)
13. [ìš´ì˜ ê°€ì´ë“œ](#13-ìš´ì˜-ê°€ì´ë“œ)

---

## 1. ì•„í‚¤í…ì²˜ ê°œìš”

### 1.1 ì „ì²´ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      AWS Cloud (Seoul)                       â”‚
â”‚                                                               â”‚
â”‚  ğŸ‘¥ Users â†’ CloudFront â†’ WAF â†’ ALB                          â”‚
â”‚                              â†“                               â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚         â”‚     Amazon EKS Cluster              â”‚              â”‚
â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚              â”‚
â”‚         â”‚  â”‚ Frontend â”‚ Backend  â”‚ GPU AI  â”‚ â”‚              â”‚
â”‚         â”‚  â”‚  Pods    â”‚  Pods    â”‚ Pods    â”‚ â”‚              â”‚
â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚              â”‚
â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚              â”‚
â”‚         â”‚  â”‚ Email    â”‚ Celery   â”‚Monitor  â”‚ â”‚              â”‚
â”‚         â”‚  â”‚ Service  â”‚ Workers  â”‚ Stack   â”‚ â”‚              â”‚
â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                      â†“         â†“         â†“                   â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚         â”‚ Aurora   â”‚ElastiCacheâ”‚   S3    â”‚    EFS   â”‚       â”‚
â”‚         â”‚PostgreSQLâ”‚  Redis   â”‚Buckets  â”‚  Share   â”‚       â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                      â†“                                        â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚         â”‚  Elasticsearch Cluster (3 nodes) â”‚                â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                               â”‚
â”‚  External: Lemon Squeezy (Payment) | SES (Email)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 ê¸°ìˆ  ìŠ¤íƒ

| ë ˆì´ì–´ | ê¸°ìˆ  | ì„¤ëª… |
|-------|------|------|
| **Frontend** | React 18 + TypeScript + Redux | SPA, Vite, TailwindCSS |
| **Backend** | FastAPI + Python 3.12 | Async, Pydantic, SQLAlchemy 2.0 |
| **AI Engine** | InvokeAI + PyTorch 2.1 | CUDA 12.1, Mixed Precision |
| **Database** | Aurora PostgreSQL 15 | Multi-AZ, Auto-scaling |
| **Cache** | ElastiCache Redis 7.0 | Session + Celery Queue |
| **Storage** | S3 + EFS | Images + Models |
| **Search** | Elasticsearch 8.11.0 | Korean (Nori) Analyzer |
| **Email** | AWS SES + Lambda | Transactional + Newsletter |
| **Payment** | Lemon Squeezy | MoR, Global Tax |
| **Orchestration** | Amazon EKS 1.31 | Kubernetes, Spot Instances |
| **CI/CD** | GitHub Actions + ArgoCD | GitOps Deployment |
| **Monitoring** | Prometheus + Grafana | In-cluster OSS |

### 1.3 ë¹„ìš© ë¶„ì„

| í•­ëª© | ì›” ë¹„ìš© | ì„¤ëª… |
|-----|--------|------|
| **CDN + Security** | $30.00 | CloudFront, WAF, Route 53, Shield |
| **Load Balancer** | $19.00 | ALB Multi-AZ |
| **EKS + Compute** | $372.00 | Control Plane ($73) + 6 CPU ($73x3) + 1 GPU ($299) |
| **Data Layer** | $314.00 | Aurora, ElastiCache, S3, EFS, NAT, Secrets |
| **Search** | $16.15 | Elasticsearch 3 nodes |
| **Email** | $7.00 | SES, SQS, Lambda |
| **Monitoring** | $5.00 | CloudWatch |
| **ì´ê³„** | **$827.15** | **ì•½ 110ë§Œì›/ì›”** |

**ë¹„ìš© ì ˆê° í¬ì¸íŠ¸:**
- Spot Instances ì‚¬ìš© (70% í• ì¸)
- In-cluster ëª¨ë‹ˆí„°ë§ (Prometheus/Grafana OSS)
- Shared Aurora (dev + prod)
- EFS Infrequent Access (30ì¼ í›„)
- S3 Intelligent-Tiering

### 1.4 ì˜ˆìƒ ROI

**ìˆ˜ìµ ì‹œë‚˜ë¦¬ì˜¤ (ë³´ìˆ˜ì ):**
- ì´ ì‚¬ìš©ì: 10,000ëª…
- ìœ ë£Œ ì „í™˜ìœ¨: 10% (1,000ëª…)
- í‰ê·  ê²°ì œ: $29/ì›”

**ì›” ìˆ˜ìµ**: $29,000
**ì›” ë¹„ìš©**: $827
**ìˆœìµ**: **$28,173** (ROI: 3,407%)

---

## 2. ì‚¬ì „ ì¤€ë¹„ (Week 0)

### 2.1 í•„ìˆ˜ ë„êµ¬ ì„¤ì¹˜

#### macOS

```bash
# Homebrew ì„¤ì¹˜ (ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ ìƒëµ)
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# í•„ìˆ˜ ë„êµ¬ ì„¤ì¹˜
brew install \
  awscli \
  terraform \
  kubectl \
  helm \
  argocd \
  jq \
  git \
  docker \
  python@3.12 \
  node@20

# Docker Desktop ì„¤ì¹˜
brew install --cask docker

# VS Code ì„¤ì¹˜ (ì„ íƒì‚¬í•­)
brew install --cask visual-studio-code
```

#### Ubuntu/Debian

```bash
# AWS CLI
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install

# Terraform
wget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg
echo "deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main" | sudo tee /etc/apt/sources.list.d/hashicorp.list
sudo apt update && sudo apt install terraform

# kubectl
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

# Helm
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# ArgoCD CLI
curl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64
sudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd

# Docker
sudo apt-get update
sudo apt-get install docker.io docker-compose
sudo usermod -aG docker $USER

# Python 3.12
sudo apt-get install python3.12 python3.12-venv python3-pip

# Node.js 20
curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash -
sudo apt-get install -y nodejs
```

#### ë²„ì „ í™•ì¸

```bash
# ëª¨ë“  ë„êµ¬ ë²„ì „ í™•ì¸
aws --version          # aws-cli/2.x.x
terraform --version    # Terraform v1.6+
kubectl version --client  # v1.28+
helm version           # v3.13+
docker --version       # 24.0+
python3.12 --version   # 3.12+
node --version         # v20.x+
```

### 2.2 AWS ê³„ì • ì„¤ì •

#### AWS ê³„ì • ìƒì„± ë° ì„¤ì •

```bash
# AWS CLI ì„¤ì •
aws configure

# ì…ë ¥ ì˜ˆì‹œ:
# AWS Access Key ID: AKIAIOSFODNN7EXAMPLE
# AWS Secret Access Key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
# Default region name: ap-northeast-2  # Seoul
# Default output format: json

# ì„¤ì • í™•ì¸
aws sts get-caller-identity

# ì¶œë ¥ ì˜ˆì‹œ:
# {
#     "UserId": "AIDAI...",
#     "Account": "123456789012",
#     "Arn": "arn:aws:iam::123456789012:user/admin"
# }
```

#### IAM ì‚¬ìš©ì ê¶Œí•œ ì„¤ì •

ìµœì†Œ í•„ìš” ê¶Œí•œ:
- AmazonEKSClusterPolicy
- AmazonEKSWorkerNodePolicy
- AmazonEC2ContainerRegistryFullAccess
- AmazonS3FullAccess
- AmazonRDSFullAccess
- AmazonElastiCacheFullAccess
- AmazonSESFullAccess
- CloudWatchFullAccess
- IAMFullAccess (ì¸í”„ë¼ ìƒì„±ìš©)

**ë³´ì•ˆ ê¶Œì¥ì‚¬í•­:**
- ë£¨íŠ¸ ê³„ì • ì§ì ‘ ì‚¬ìš© ê¸ˆì§€
- IAM ì‚¬ìš©ì + MFA í™œì„±í™”
- ìµœì†Œ ê¶Œí•œ ì›ì¹™ ì ìš©

### 2.3 í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„±

```bash
# í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p ~/pingvasai-saas
cd ~/pingvasai-saas

# ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
mkdir -p {terraform,k8s,docker,scripts,docs}
mkdir -p k8s/{base,overlays/{dev,prod}}
mkdir -p terraform/{modules,environments/{dev,prod}}

# Git ì´ˆê¸°í™”
git init
git remote add origin https://github.com/your-org/pingvasai-saas.git

# .gitignore ìƒì„±
cat > .gitignore << 'EOF'
# Terraform
*.tfstate
*.tfstate.*
.terraform/
.terraform.lock.hcl

# Kubernetes
*.kubeconfig

# Python
__pycache__/
*.py[cod]
*$py.class
venv/
.env

# Secrets
*.pem
*.key
secrets.yaml
.env.local
.env.production

# IDE
.vscode/
.idea/
*.swp
*.swo

# OS
.DS_Store
Thumbs.db
EOF
```

### 2.4 InvokeAI ì†ŒìŠ¤ì½”ë“œ í´ë¡ 

```bash
# InvokeAI í´ë¡ 
cd ~/pingvasai-saas
git clone https://github.com/invoke-ai/InvokeAI.git original-invokeai

# ìš°ë¦¬ í”„ë¡œì íŠ¸ë¡œ ë³µì‚¬
cp -r original-invokeai/invokeai ./backend/
cp -r original-invokeai/invokeai/frontend ./frontend/

# Python ì˜ì¡´ì„± í™•ì¸
cd backend
cat requirements.txt

# Node.js ì˜ì¡´ì„± í™•ì¸
cd ../frontend
cat package.json
```

### 2.5 ì²´í¬ë¦¬ìŠ¤íŠ¸

ì™„ë£Œí•œ í•­ëª©ì— ì²´í¬í•˜ì„¸ìš”:

- [ ] AWS CLI ì„¤ì¹˜ ë° ì„¤ì • ì™„ë£Œ
- [ ] Terraform ì„¤ì¹˜ ì™„ë£Œ
- [ ] kubectl ì„¤ì¹˜ ì™„ë£Œ
- [ ] Helm ì„¤ì¹˜ ì™„ë£Œ
- [ ] Docker ì„¤ì¹˜ ë° ì‹¤í–‰ í™•ì¸
- [ ] AWS ê³„ì • ìƒì„± ë° IAM ê¶Œí•œ ì„¤ì •
- [ ] í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
- [ ] InvokeAI ì†ŒìŠ¤ì½”ë“œ í´ë¡  ì™„ë£Œ
- [ ] Git ì €ì¥ì†Œ ì´ˆê¸°í™”
- [ ] íŒ€ì›ë“¤ê³¼ Git ì €ì¥ì†Œ ê³µìœ 

---

## 3. Phase 1: ê¸°ì´ˆ ì¸í”„ë¼ êµ¬ì¶• (Week 1-2)

**ëª©í‘œ**: VPC, Networking, ê¸°ë³¸ AWS ì„œë¹„ìŠ¤ ì„¤ì •
**ì˜ˆìƒ ì‹œê°„**: 2ì£¼
**ë¹„ìš©**: ~$0 (í”„ë¦¬ í‹°ì–´ ë‚´)

### 3.1 Terraform ë°±ì—”ë“œ ì„¤ì •

#### S3 ë²„í‚· ìƒì„± (Terraform State ì €ì¥ìš©)

```bash
cd ~/pingvasai-saas/terraform

# S3 ë²„í‚· ìƒì„± (ìˆ˜ë™, í•œ ë²ˆë§Œ)
aws s3api create-bucket \
  --bucket pingvasai-terraform-state \
  --region ap-northeast-2 \
  --create-bucket-configuration LocationConstraint=ap-northeast-2

# ë²„ì „ ê´€ë¦¬ í™œì„±í™”
aws s3api put-bucket-versioning \
  --bucket pingvasai-terraform-state \
  --versioning-configuration Status=Enabled

# ì•”í˜¸í™” í™œì„±í™”
aws s3api put-bucket-encryption \
  --bucket pingvasai-terraform-state \
  --server-side-encryption-configuration '{
    "Rules": [{
      "ApplyServerSideEncryptionByDefault": {
        "SSEAlgorithm": "AES256"
      }
    }]
  }'

# DynamoDB í…Œì´ë¸” ìƒì„± (State Lockìš©)
aws dynamodb create-table \
  --table-name pingvasai-terraform-locks \
  --attribute-definitions AttributeName=LockID,AttributeType=S \
  --key-schema AttributeName=LockID,KeyType=HASH \
  --billing-mode PAY_PER_REQUEST \
  --region ap-northeast-2
```

#### Terraform ë°±ì—”ë“œ ì„¤ì •

```hcl
# terraform/backend.tf
terraform {
  backend "s3" {
    bucket         = "pingvasai-terraform-state"
    key            = "production/terraform.tfstate"
    region         = "ap-northeast-2"
    encrypt        = true
    dynamodb_table = "pingvasai-terraform-locks"
  }

  required_version = ">= 1.6.0"

  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.24"
    }
    helm = {
      source  = "hashicorp/helm"
      version = "~> 2.12"
    }
  }
}
```

### 3.2 VPC ë° ë„¤íŠ¸ì›Œí¬ êµ¬ì„±

```hcl
# terraform/modules/vpc/main.tf

# VPC ìƒì„±
resource "aws_vpc" "main" {
  cidr_block           = "10.0.0.0/16"
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name        = "pingvasai-vpc"
    Environment = var.environment
    Terraform   = "true"
  }
}

# Internet Gateway
resource "aws_internet_gateway" "main" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = "pingvasai-igw"
  }
}

# Public Subnets (3 AZs)
resource "aws_subnet" "public" {
  count = 3

  vpc_id                  = aws_vpc.main.id
  cidr_block              = "10.0.${count.index}.0/24"
  availability_zone       = data.aws_availability_zones.available.names[count.index]
  map_public_ip_on_launch = true

  tags = {
    Name                                           = "pingvasai-public-${count.index + 1}"
    "kubernetes.io/role/elb"                       = "1"
    "kubernetes.io/cluster/pingvasai-eks-cluster" = "shared"
  }
}

# Private Subnets - App (3 AZs)
resource "aws_subnet" "private_app" {
  count = 3

  vpc_id            = aws_vpc.main.id
  cidr_block        = "10.0.${count.index + 10}.0/24"
  availability_zone = data.aws_availability_zones.available.names[count.index]

  tags = {
    Name                                           = "pingvasai-private-app-${count.index + 1}"
    "kubernetes.io/role/internal-elb"              = "1"
    "kubernetes.io/cluster/pingvasai-eks-cluster" = "shared"
  }
}

# Private Subnets - Database (3 AZs)
resource "aws_subnet" "private_db" {
  count = 3

  vpc_id            = aws_vpc.main.id
  cidr_block        = "10.0.${count.index + 20}.0/24"
  availability_zone = data.aws_availability_zones.available.names[count.index]

  tags = {
    Name = "pingvasai-private-db-${count.index + 1}"
  }
}

# Elastic IPs for NAT Gateways
resource "aws_eip" "nat" {
  count  = 3
  domain = "vpc"

  tags = {
    Name = "pingvasai-nat-eip-${count.index + 1}"
  }

  depends_on = [aws_internet_gateway.main]
}

# NAT Gateways (3 AZs for HA)
resource "aws_nat_gateway" "main" {
  count = 3

  allocation_id = aws_eip.nat[count.index].id
  subnet_id     = aws_subnet.public[count.index].id

  tags = {
    Name = "pingvasai-nat-${count.index + 1}"
  }

  depends_on = [aws_internet_gateway.main]
}

# Route Tables - Public
resource "aws_route_table" "public" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.main.id
  }

  tags = {
    Name = "pingvasai-public-rt"
  }
}

# Route Table Associations - Public
resource "aws_route_table_association" "public" {
  count = 3

  subnet_id      = aws_subnet.public[count.index].id
  route_table_id = aws_route_table.public.id
}

# Route Tables - Private App (ê° AZë³„ë¡œ NAT Gateway ì‚¬ìš©)
resource "aws_route_table" "private_app" {
  count = 3

  vpc_id = aws_vpc.main.id

  route {
    cidr_block     = "0.0.0.0/0"
    nat_gateway_id = aws_nat_gateway.main[count.index].id
  }

  tags = {
    Name = "pingvasai-private-app-rt-${count.index + 1}"
  }
}

# Route Table Associations - Private App
resource "aws_route_table_association" "private_app" {
  count = 3

  subnet_id      = aws_subnet.private_app[count.index].id
  route_table_id = aws_route_table.private_app[count.index].id
}

# Route Tables - Private DB
resource "aws_route_table" "private_db" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = "pingvasai-private-db-rt"
  }
}

# Route Table Associations - Private DB
resource "aws_route_table_association" "private_db" {
  count = 3

  subnet_id      = aws_subnet.private_db[count.index].id
  route_table_id = aws_route_table.private_db.id
}

# VPC Endpoints (ë¹„ìš© ì ˆê°)
resource "aws_vpc_endpoint" "s3" {
  vpc_id       = aws_vpc.main.id
  service_name = "com.amazonaws.ap-northeast-2.s3"

  tags = {
    Name = "pingvasai-s3-endpoint"
  }
}

resource "aws_vpc_endpoint_route_table_association" "s3_private_app" {
  count = 3

  route_table_id  = aws_route_table.private_app[count.index].id
  vpc_endpoint_id = aws_vpc_endpoint.s3.id
}

# Data Sources
data "aws_availability_zones" "available" {
  state = "available"
}

# Outputs
output "vpc_id" {
  value = aws_vpc.main.id
}

output "public_subnet_ids" {
  value = aws_subnet.public[*].id
}

output "private_app_subnet_ids" {
  value = aws_subnet.private_app[*].id
}

output "private_db_subnet_ids" {
  value = aws_subnet.private_db[*].id
}
```

### 3.3 Security Groups

```hcl
# terraform/modules/security_groups/main.tf

# ALB Security Group
resource "aws_security_group" "alb" {
  name        = "pingvasai-alb-sg"
  description = "Security group for ALB"
  vpc_id      = var.vpc_id

  ingress {
    description = "HTTP from Internet"
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    description = "HTTPS from Internet"
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    description = "All outbound traffic"
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "pingvasai-alb-sg"
  }
}

# EKS Node Security Group
resource "aws_security_group" "eks_nodes" {
  name        = "pingvasai-eks-nodes-sg"
  description = "Security group for EKS worker nodes"
  vpc_id      = var.vpc_id

  ingress {
    description     = "Allow nodes to communicate with each other"
    from_port       = 0
    to_port         = 65535
    protocol        = "-1"
    self            = true
  }

  ingress {
    description     = "Allow ALB to reach pods"
    from_port       = 0
    to_port         = 65535
    protocol        = "-1"
    security_groups = [aws_security_group.alb.id]
  }

  egress {
    description = "All outbound traffic"
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "pingvasai-eks-nodes-sg"
  }
}

# RDS Security Group
resource "aws_security_group" "rds" {
  name        = "pingvasai-rds-sg"
  description = "Security group for RDS"
  vpc_id      = var.vpc_id

  ingress {
    description     = "PostgreSQL from EKS nodes"
    from_port       = 5432
    to_port         = 5432
    protocol        = "tcp"
    security_groups = [aws_security_group.eks_nodes.id]
  }

  egress {
    description = "All outbound traffic"
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "pingvasai-rds-sg"
  }
}

# ElastiCache Security Group
resource "aws_security_group" "elasticache" {
  name        = "pingvasai-elasticache-sg"
  description = "Security group for ElastiCache Redis"
  vpc_id      = var.vpc_id

  ingress {
    description     = "Redis from EKS nodes"
    from_port       = 6379
    to_port         = 6379
    protocol        = "tcp"
    security_groups = [aws_security_group.eks_nodes.id]
  }

  egress {
    description = "All outbound traffic"
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "pingvasai-elasticache-sg"
  }
}

# EFS Security Group
resource "aws_security_group" "efs" {
  name        = "pingvasai-efs-sg"
  description = "Security group for EFS"
  vpc_id      = var.vpc_id

  ingress {
    description     = "NFS from EKS nodes"
    from_port       = 2049
    to_port         = 2049
    protocol        = "tcp"
    security_groups = [aws_security_group.eks_nodes.id]
  }

  egress {
    description = "All outbound traffic"
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "pingvasai-efs-sg"
  }
}

# Elasticsearch Security Group
resource "aws_security_group" "elasticsearch" {
  name        = "pingvasai-elasticsearch-sg"
  description = "Security group for Elasticsearch"
  vpc_id      = var.vpc_id

  ingress {
    description     = "Elasticsearch from EKS nodes"
    from_port       = 9200
    to_port         = 9200
    protocol        = "tcp"
    security_groups = [aws_security_group.eks_nodes.id]
  }

  ingress {
    description     = "Elasticsearch transport from EKS nodes"
    from_port       = 9300
    to_port         = 9300
    protocol        = "tcp"
    security_groups = [aws_security_group.eks_nodes.id]
  }

  egress {
    description = "All outbound traffic"
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "pingvasai-elasticsearch-sg"
  }
}
```

### 3.4 ì¸í”„ë¼ ë°°í¬

```bash
cd ~/pingvasai-saas/terraform

# Terraform ì´ˆê¸°í™”
terraform init

# ê³„íš í™•ì¸
terraform plan -out=tfplan

# ê²€í†  í›„ ì ìš©
terraform apply tfplan

# ì¶œë ¥ í™•ì¸
terraform output

# ì˜ˆìƒ ì¶œë ¥:
# vpc_id = "vpc-xxxxx"
# public_subnet_ids = ["subnet-xxxxx", "subnet-yyyyy", "subnet-zzzzz"]
# private_app_subnet_ids = ["subnet-aaaaa", "subnet-bbbbb", "subnet-ccccc"]
```

### 3.5 ê²€ì¦

```bash
# VPC í™•ì¸
aws ec2 describe-vpcs --filters "Name=tag:Name,Values=pingvasai-vpc"

# Subnets í™•ì¸
aws ec2 describe-subnets --filters "Name=vpc-id,Values=$(terraform output -raw vpc_id)"

# NAT Gateways í™•ì¸
aws ec2 describe-nat-gateways --filter "Name=vpc-id,Values=$(terraform output -raw vpc_id)"

# Security Groups í™•ì¸
aws ec2 describe-security-groups --filters "Name=vpc-id,Values=$(terraform output -raw vpc_id)"
```

### 3.6 Phase 1 ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Terraform ë°±ì—”ë“œ (S3 + DynamoDB) ìƒì„±
- [ ] VPC ìƒì„± (10.0.0.0/16)
- [ ] Public Subnets 3ê°œ ìƒì„±
- [ ] Private App Subnets 3ê°œ ìƒì„±
- [ ] Private DB Subnets 3ê°œ ìƒì„±
- [ ] NAT Gateways 3ê°œ ìƒì„±
- [ ] Security Groups ìƒì„± (ALB, EKS, RDS, ElastiCache, EFS, ES)
- [ ] VPC Endpoints ìƒì„± (S3)
- [ ] ë„¤íŠ¸ì›Œí¬ ì—°ê²° í…ŒìŠ¤íŠ¸ ì™„ë£Œ

---

## 4. Phase 2: EKS í´ëŸ¬ìŠ¤í„° ì„¤ì • (Week 3-4)

**ëª©í‘œ**: EKS í´ëŸ¬ìŠ¤í„° ìƒì„± ë° ë…¸ë“œ ê·¸ë£¹ ì„¤ì •
**ì˜ˆìƒ ì‹œê°„**: 2ì£¼
**ë¹„ìš©**: ~$73/ì›” (Control Plane)

### 4.1 EKS í´ëŸ¬ìŠ¤í„° ìƒì„±

```hcl
# terraform/modules/eks/main.tf

module "eks" {
  source  = "terraform-aws-modules/eks/aws"
  version = "~> 19.0"

  cluster_name    = "pingvasai-eks-cluster"
  cluster_version = "1.31"

  vpc_id                   = var.vpc_id
  subnet_ids               = var.private_app_subnet_ids
  control_plane_subnet_ids = var.public_subnet_ids

  # í´ëŸ¬ìŠ¤í„° ì—”ë“œí¬ì¸íŠ¸ ì„¤ì •
  cluster_endpoint_public_access  = true
  cluster_endpoint_private_access = true

  # í´ëŸ¬ìŠ¤í„° ì• ë“œì˜¨
  cluster_addons = {
    coredns = {
      most_recent = true
    }
    kube-proxy = {
      most_recent = true
    }
    vpc-cni = {
      most_recent = true
    }
    aws-ebs-csi-driver = {
      most_recent = true
    }
  }

  # EKS Managed Node Groups
  eks_managed_node_groups = {
    # General Purpose Nodes (CPU ì›Œí¬ë¡œë“œ)
    general = {
      name = "general-ng"

      instance_types = ["t3.medium"]
      capacity_type  = "SPOT"  # 70% í• ì¸

      min_size     = 3
      max_size     = 10
      desired_size = 3

      disk_size = 50  # GB

      labels = {
        role = "general"
      }

      tags = {
        "k8s.io/cluster-autoscaler/enabled"                   = "true"
        "k8s.io/cluster-autoscaler/pingvasai-eks-cluster"    = "owned"
      }
    }

    # GPU Nodes (AI ì›Œí¬ë¡œë“œ)
    gpu = {
      name = "gpu-ng"

      instance_types = ["g5.xlarge"]  # A10G 24GB VRAM
      capacity_type  = "ON_DEMAND"     # Hot ë…¸ë“œëŠ” On-Demand

      min_size     = 1
      max_size     = 5
      desired_size = 1

      disk_size = 100  # GB (ëª¨ë¸ ìºì‹œìš©)

      # GPU ë…¸ë“œëŠ” taint ì„¤ì • (GPU ì‘ì—…ë§Œ ìŠ¤ì¼€ì¤„ë§)
      taints = [{
        key    = "nvidia.com/gpu"
        value  = "true"
        effect = "NoSchedule"
      }]

      labels = {
        role = "gpu-worker"
        "nvidia.com/gpu" = "true"
      }

      tags = {
        "k8s.io/cluster-autoscaler/enabled"                   = "true"
        "k8s.io/cluster-autoscaler/pingvasai-eks-cluster"    = "owned"
      }
    }
  }

  # IRSA (IAM Roles for Service Accounts)
  enable_irsa = true

  # í´ëŸ¬ìŠ¤í„° ë³´ì•ˆ ê·¸ë£¹ ê·œì¹™
  cluster_security_group_additional_rules = {
    ingress_nodes_ephemeral_ports_tcp = {
      description                = "Nodes on ephemeral ports"
      protocol                   = "tcp"
      from_port                  = 1025
      to_port                    = 65535
      type                       = "ingress"
      source_node_security_group = true
    }
  }

  # ë…¸ë“œ ë³´ì•ˆ ê·¸ë£¹ ê·œì¹™
  node_security_group_additional_rules = {
    ingress_self_all = {
      description = "Node to node all ports/protocols"
      protocol    = "-1"
      from_port   = 0
      to_port     = 0
      type        = "ingress"
      self        = true
    }
    ingress_alb_all = {
      description              = "ALB to node all ports"
      protocol                 = "-1"
      from_port                = 0
      to_port                  = 0
      type                     = "ingress"
      source_security_group_id = var.alb_security_group_id
    }
  }

  tags = {
    Environment = "production"
    Terraform   = "true"
  }
}

# Outputs
output "cluster_id" {
  value = module.eks.cluster_id
}

output "cluster_endpoint" {
  value = module.eks.cluster_endpoint
}

output "cluster_security_group_id" {
  value = module.eks.cluster_security_group_id
}

output "cluster_certificate_authority_data" {
  value = module.eks.cluster_certificate_authority_data
}
```

### 4.2 EKS í´ëŸ¬ìŠ¤í„° ë°°í¬

```bash
cd ~/pingvasai-saas/terraform

# EKS ëª¨ë“ˆ ì¶”ê°€ í›„ ì ìš©
terraform plan -out=tfplan
terraform apply tfplan

# ì™„ë£Œê¹Œì§€ ì•½ 15-20ë¶„ ì†Œìš”
```

### 4.3 kubectl ì„¤ì •

```bash
# kubeconfig ì—…ë°ì´íŠ¸
aws eks update-kubeconfig \
  --region ap-northeast-2 \
  --name pingvasai-eks-cluster

# í´ëŸ¬ìŠ¤í„° ì—°ê²° í™•ì¸
kubectl cluster-info

# ë…¸ë“œ í™•ì¸
kubectl get nodes

# ì˜ˆìƒ ì¶œë ¥:
# NAME                                           STATUS   ROLES    AGE   VERSION
# ip-10-0-10-xxx.ap-northeast-2.compute.internal   Ready    <none>   5m    v1.31.0-eks-xxxxx
# ip-10-0-11-xxx.ap-northeast-2.compute.internal   Ready    <none>   5m    v1.31.0-eks-xxxxx
# ip-10-0-12-xxx.ap-northeast-2.compute.internal   Ready    <none>   5m    v1.31.0-eks-xxxxx
# ip-10-0-10-yyy.ap-northeast-2.compute.internal   Ready    <none>   5m    v1.31.0-eks-xxxxx  # GPU Node
```

### 4.4 ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±

```bash
# ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
kubectl create namespace dev
kubectl create namespace prod
kubectl create namespace ingress-nginx
kubectl create namespace monitoring
kubectl create namespace argocd

# ë„¤ì„ìŠ¤í˜ì´ìŠ¤ í™•ì¸
kubectl get namespaces

# ê¸°ë³¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì„¤ì • (ê°œë°œ ì‹œ)
kubectl config set-context --current --namespace=dev
```

### 4.5 NVIDIA Device Plugin ì„¤ì¹˜ (GPU ì§€ì›)

```bash
# NVIDIA Device Plugin ë°°í¬
kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.14.3/nvidia-device-plugin.yml

# í™•ì¸
kubectl get pods -n kube-system | grep nvidia-device-plugin

# GPU ë¦¬ì†ŒìŠ¤ í™•ì¸
kubectl describe node <gpu-node-name> | grep nvidia.com/gpu

# ì˜ˆìƒ ì¶œë ¥:
# nvidia.com/gpu:     1
```

### 4.6 EBS CSI Driver ì„¤ì •

```bash
# IAM ì •ì±… ìƒì„± (Terraformìœ¼ë¡œ ìë™ ìƒì„±ë¨)
# EBS CSI DriverëŠ” EKS ì• ë“œì˜¨ìœ¼ë¡œ ì´ë¯¸ ì„¤ì¹˜ë¨

# StorageClass í™•ì¸
kubectl get storageclass

# gp3 StorageClass ìƒì„± (ê¸°ë³¸ê°’)
cat > ebs-gp3-storageclass.yaml << 'EOF'
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gp3
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: ebs.csi.aws.com
parameters:
  type: gp3
  iops: "3000"
  throughput: "125"
  encrypted: "true"
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
EOF

kubectl apply -f ebs-gp3-storageclass.yaml
```

### 4.7 EFS CSI Driver ì„¤ì¹˜

```hcl
# terraform/modules/efs/main.tf

# EFS íŒŒì¼ ì‹œìŠ¤í…œ ìƒì„±
resource "aws_efs_file_system" "models" {
  creation_token = "pingvasai-models"
  encrypted      = true

  performance_mode = "generalPurpose"
  throughput_mode  = "bursting"

  lifecycle_policy {
    transition_to_ia = "AFTER_30_DAYS"  # Infrequent Accessë¡œ ì´ë™
  }

  tags = {
    Name = "pingvasai-models-efs"
  }
}

# EFS ë§ˆìš´íŠ¸ íƒ€ê²Ÿ (ê° AZ)
resource "aws_efs_mount_target" "models" {
  count = 3

  file_system_id  = aws_efs_file_system.models.id
  subnet_id       = var.private_app_subnet_ids[count.index]
  security_groups = [var.efs_security_group_id]
}

# Outputs
output "efs_id" {
  value = aws_efs_file_system.models.id
}
```

```bash
# EFS CSI Driver ì„¤ì¹˜
helm repo add aws-efs-csi-driver https://kubernetes-sigs.github.io/aws-efs-csi-driver/
helm repo update

helm install aws-efs-csi-driver aws-efs-csi-driver/aws-efs-csi-driver \
  --namespace kube-system \
  --set controller.serviceAccount.create=true \
  --set controller.serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):role/EFSCSIDriverRole"

# StorageClass ìƒì„±
cat > efs-storageclass.yaml << EOF
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: efs
provisioner: efs.csi.aws.com
parameters:
  provisioningMode: efs-ap
  fileSystemId: $(terraform output -raw efs_id)
  directoryPerms: "700"
EOF

kubectl apply -f efs-storageclass.yaml
```

### 4.8 Cluster Autoscaler ì„¤ì¹˜

```bash
# Helmìœ¼ë¡œ Cluster Autoscaler ì„¤ì¹˜
helm repo add autoscaler https://kubernetes.github.io/autoscaler
helm repo update

helm install cluster-autoscaler autoscaler/cluster-autoscaler \
  --namespace kube-system \
  --set autoDiscovery.clusterName=pingvasai-eks-cluster \
  --set awsRegion=ap-northeast-2 \
  --set rbac.serviceAccount.create=true \
  --set rbac.serviceAccount.name=cluster-autoscaler \
  --set rbac.serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):role/ClusterAutoscalerRole" \
  --set extraArgs.balance-similar-node-groups=true \
  --set extraArgs.skip-nodes-with-system-pods=false

# í™•ì¸
kubectl get pods -n kube-system | grep cluster-autoscaler
```

### 4.9 Phase 2 ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] EKS í´ëŸ¬ìŠ¤í„° ìƒì„± ì™„ë£Œ (1.31)
- [ ] kubectl ì„¤ì • ë° í´ëŸ¬ìŠ¤í„° ì ‘ê·¼ í™•ì¸
- [ ] General Purpose Node Group ìƒì„± (3 nodes, t3.medium Spot)
- [ ] GPU Node Group ìƒì„± (1 node, g5.xlarge On-Demand)
- [ ] ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„± (dev, prod, ingress-nginx, monitoring, argocd)
- [ ] NVIDIA Device Plugin ì„¤ì¹˜ ë° GPU ì¸ì‹ í™•ì¸
- [ ] EBS CSI Driver ì„¤ì¹˜ ë° gp3 StorageClass ìƒì„±
- [ ] EFS ìƒì„± ë° EFS CSI Driver ì„¤ì¹˜
- [ ] Cluster Autoscaler ì„¤ì¹˜

---

## 5. Phase 3: ì• í”Œë¦¬ì¼€ì´ì…˜ ë§ˆì´ê·¸ë ˆì´ì…˜ (Week 5-6)

**ëª©í‘œ**: InvokeAIë¥¼ Kubernetesì— ë°°í¬
**ì˜ˆìƒ ì‹œê°„**: 2ì£¼
**ë¹„ìš©**: +$144/ì›” (Frontend + Backend Pods)

### 5.1 Docker ì´ë¯¸ì§€ ë¹Œë“œ

#### Backend Dockerfile

```dockerfile
# docker/backend/Dockerfile

FROM python:3.12-slim

WORKDIR /app

# ì‹œìŠ¤í…œ ì˜ì¡´ì„± ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    git \
    libpq-dev \
    && rm -rf /var/lib/apt/lists/*

# Python ì˜ì¡´ì„± ì„¤ì¹˜
COPY backend/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY backend/ .

# í™˜ê²½ ë³€ìˆ˜
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1

# í—¬ìŠ¤ì²´í¬
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
  CMD python -c "import requests; requests.get('http://localhost:9090/api/v1/health')"

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰
CMD ["uvicorn", "invokeai.app.run_app:app", "--host", "0.0.0.0", "--port", "9090"]
```

#### Frontend Dockerfile

```dockerfile
# docker/frontend/Dockerfile

# Build Stage
FROM node:20-alpine AS builder

WORKDIR /app

# ì˜ì¡´ì„± ì„¤ì¹˜
COPY frontend/package*.json ./
RUN npm ci

# ì• í”Œë¦¬ì¼€ì´ì…˜ ë¹Œë“œ
COPY frontend/ .
RUN npm run build

# Production Stage
FROM nginx:alpine

# Nginx ì„¤ì • ë³µì‚¬
COPY docker/frontend/nginx.conf /etc/nginx/nginx.conf

# ë¹Œë“œëœ íŒŒì¼ ë³µì‚¬
COPY --from=builder /app/dist /usr/share/nginx/html

# í—¬ìŠ¤ì²´í¬
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD wget --quiet --tries=1 --spider http://localhost:80/ || exit 1

EXPOSE 80

CMD ["nginx", "-g", "daemon off;"]
```

#### Nginx ì„¤ì •

```nginx
# docker/frontend/nginx.conf

user  nginx;
worker_processes  auto;

error_log  /var/log/nginx/error.log warn;
pid        /var/run/nginx.pid;

events {
    worker_connections  1024;
}

http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';

    access_log  /var/log/nginx/access.log  main;

    sendfile        on;
    tcp_nopush     on;
    tcp_nodelay    on;
    keepalive_timeout  65;
    types_hash_max_size 2048;

    gzip  on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_types text/plain text/css text/xml text/javascript application/x-javascript application/xml+rss application/javascript application/json;

    server {
        listen       80;
        server_name  localhost;

        root   /usr/share/nginx/html;
        index  index.html index.htm;

        # React Router ì§€ì› (SPA)
        location / {
            try_files $uri $uri/ /index.html;
        }

        # API í”„ë¡ì‹œ
        location /api/ {
            proxy_pass http://api-service.prod.svc.cluster.local:9090/api/;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection 'upgrade';
            proxy_set_header Host $host;
            proxy_cache_bypass $http_upgrade;
        }

        # ì •ì  íŒŒì¼ ìºì‹±
        location ~* \.(jpg|jpeg|png|gif|ico|css|js|svg|woff|woff2|ttf|eot)$ {
            expires 1y;
            add_header Cache-Control "public, immutable";
        }

        # í—¬ìŠ¤ì²´í¬
        location /health {
            access_log off;
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }
    }
}
```

#### ì´ë¯¸ì§€ ë¹Œë“œ ë° í‘¸ì‹œ

```bash
# ECR ë¡œê·¸ì¸
aws ecr get-login-password --region ap-northeast-2 | \
  docker login --username AWS --password-stdin \
  $(aws sts get-caller-identity --query Account --output text).dkr.ecr.ap-northeast-2.amazonaws.com

# ECR ë ˆí¬ì§€í† ë¦¬ ìƒì„±
aws ecr create-repository --repository-name pingvasai/backend --region ap-northeast-2
aws ecr create-repository --repository-name pingvasai/frontend --region ap-northeast-2

# Backend ì´ë¯¸ì§€ ë¹Œë“œ
cd ~/pingvasai-saas
docker build -t pingvasai/backend:latest -f docker/backend/Dockerfile .

# Backend ì´ë¯¸ì§€ íƒœê·¸ ë° í‘¸ì‹œ
ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
docker tag pingvasai/backend:latest \
  $ACCOUNT_ID.dkr.ecr.ap-northeast-2.amazonaws.com/pingvasai/backend:latest

docker push $ACCOUNT_ID.dkr.ecr.ap-northeast-2.amazonaws.com/pingvasai/backend:latest

# Frontend ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t pingvasai/frontend:latest -f docker/frontend/Dockerfile .

# Frontend ì´ë¯¸ì§€ íƒœê·¸ ë° í‘¸ì‹œ
docker tag pingvasai/frontend:latest \
  $ACCOUNT_ID.dkr.ecr.ap-northeast-2.amazonaws.com/pingvasai/frontend:latest

docker push $ACCOUNT_ID.dkr.ecr.ap-northeast-2.amazonaws.com/pingvasai/frontend:latest
```

### 5.2 Kubernetes Manifests

#### ConfigMap (í™˜ê²½ ë³€ìˆ˜)

```yaml
# k8s/base/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: pingvasai-config
data:
  # ë°ì´í„°ë² ì´ìŠ¤
  DB_HOST: "pingvasai-aurora.cluster-xxxxx.ap-northeast-2.rds.amazonaws.com"
  DB_PORT: "5432"
  DB_NAME: "pingvasai"

  # Redis
  REDIS_HOST: "pingvasai-redis.xxxxx.apne2.cache.amazonaws.com"
  REDIS_PORT: "6379"

  # S3
  S3_BUCKET: "pingvasai-images"
  S3_REGION: "ap-northeast-2"

  # ì• í”Œë¦¬ì¼€ì´ì…˜
  APP_ENV: "production"
  LOG_LEVEL: "INFO"
```

#### Secret

```bash
# Secrets ìƒì„± (ìˆ˜ë™, í•œ ë²ˆë§Œ)
kubectl create secret generic db-credentials \
  --from-literal=DB_USER=pingvasai_prod \
  --from-literal=DB_PASSWORD='SecurePassword123!' \
  -n prod

kubectl create secret generic redis-password \
  --from-literal=REDIS_PASSWORD='RedisPassword123!' \
  -n prod

# JWT Secret
kubectl create secret generic jwt-secret \
  --from-literal=JWT_SECRET_KEY=$(openssl rand -hex 32) \
  -n prod
```

#### Backend Deployment

```yaml
# k8s/base/backend-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
  labels:
    app: api-server
spec:
  replicas: 3
  selector:
    matchLabels:
      app: api-server

  template:
    metadata:
      labels:
        app: api-server
    spec:
      serviceAccountName: api-server-sa

      containers:
        - name: api
          image: ACCOUNT_ID.dkr.ecr.ap-northeast-2.amazonaws.com/pingvasai/backend:latest

          ports:
            - containerPort: 9090
              name: http

          envFrom:
            - configMapRef:
                name: pingvasai-config
            - secretRef:
                name: db-credentials
            - secretRef:
                name: redis-password
            - secretRef:
                name: jwt-secret

          resources:
            requests:
              cpu: "500m"
              memory: "1Gi"
            limits:
              cpu: "2000m"
              memory: "2Gi"

          livenessProbe:
            httpGet:
              path: /api/v1/health
              port: 9090
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3

          readinessProbe:
            httpGet:
              path: /api/v1/health
              port: 9090
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 3

---
apiVersion: v1
kind: Service
metadata:
  name: api-service
  labels:
    app: api-server
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 9090
      protocol: TCP
      name: http
  selector:
    app: api-server
```

#### Frontend Deployment

```yaml
# k8s/base/frontend-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  labels:
    app: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: frontend

  template:
    metadata:
      labels:
        app: frontend
    spec:
      containers:
        - name: nginx
          image: ACCOUNT_ID.dkr.ecr.ap-northeast-2.amazonaws.com/pingvasai/frontend:latest

          ports:
            - containerPort: 80
              name: http

          resources:
            requests:
              cpu: "200m"
              memory: "256Mi"
            limits:
              cpu: "1000m"
              memory: "512Mi"

          livenessProbe:
            httpGet:
              path: /health
              port: 80
            initialDelaySeconds: 10
            periodSeconds: 10

          readinessProbe:
            httpGet:
              path: /health
              port: 80
            initialDelaySeconds: 5
            periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: frontend-service
  labels:
    app: frontend
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
  selector:
    app: frontend
```

#### HPA (Horizontal Pod Autoscaler)

```yaml
# k8s/base/hpa.yaml

# Backend HPA
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-server-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-server

  minReplicas: 2
  maxReplicas: 6

  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70

    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80

  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 50
          periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
        - type: Percent
          value: 100
          periodSeconds: 15

---
# Frontend HPA
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: frontend-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: frontend

  minReplicas: 2
  maxReplicas: 6

  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
```

### 5.3 ë°°í¬

```bash
# ë„¤ì„ìŠ¤í˜ì´ìŠ¤ í™•ì¸
kubectl config set-context --current --namespace=prod

# ConfigMap ì ìš©
kubectl apply -f k8s/base/configmap.yaml -n prod

# Backend ë°°í¬
kubectl apply -f k8s/base/backend-deployment.yaml -n prod

# Frontend ë°°í¬
kubectl apply -f k8s/base/frontend-deployment.yaml -n prod

# HPA ë°°í¬
kubectl apply -f k8s/base/hpa.yaml -n prod

# ë°°í¬ ìƒíƒœ í™•ì¸
kubectl get deployments -n prod
kubectl get pods -n prod
kubectl get svc -n prod
kubectl get hpa -n prod

# ë¡œê·¸ í™•ì¸
kubectl logs -f deployment/api-server -n prod
kubectl logs -f deployment/frontend -n prod
```

### 5.4 Phase 3 ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Backend Dockerfile ì‘ì„±
- [ ] Frontend Dockerfile ì‘ì„±
- [ ] ECRì— ì´ë¯¸ì§€ í‘¸ì‹œ
- [ ] ConfigMap ìƒì„±
- [ ] Secrets ìƒì„±
- [ ] Backend Deployment ë°°í¬
- [ ] Frontend Deployment ë°°í¬
- [ ] Service ìƒì„±
- [ ] HPA ì„¤ì •
- [ ] Podê°€ Running ìƒíƒœ í™•ì¸
- [ ] ë¡œê·¸ í™•ì¸ (ì—ëŸ¬ ì—†ìŒ)

---

**ì´ ê°€ì´ë“œëŠ” ê³„ì† ì‘ì„± ì¤‘ì…ë‹ˆë‹¤. ë‹¤ìŒ Phaseë“¤ì´ í¬í•¨ë©ë‹ˆë‹¤:**
- Phase 4: ë©€í‹°í…Œë„Œì‹œ & ì¸ì¦ (Week 7-8)
- Phase 5: ê²°ì œ ì‹œìŠ¤í…œ (Lemon Squeezy) (Week 9-10)
- Phase 6: ì´ë©”ì¼ ì„œë¹„ìŠ¤ (AWS SES) (Week 11)
- Phase 7: ê²€ìƒ‰ ì„œë¹„ìŠ¤ (Elasticsearch) (Week 12)
- Phase 8: ëª¨ë‹ˆí„°ë§ & CI/CD (Week 13-14)

**ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? ë‹¤ìŒ Phaseë¥¼ ì‘ì„±í•´ë“œë¦´ê¹Œìš”?**
