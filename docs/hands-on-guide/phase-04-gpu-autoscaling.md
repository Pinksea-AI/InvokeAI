# Phase 4: GPU ì˜¤í† ìŠ¤ì¼€ì¼ë§

ì´ ê°€ì´ë“œëŠ” Karpenterë¥¼ ì‚¬ìš©í•œ GPU ë…¸ë“œ ë™ì  í”„ë¡œë¹„ì €ë‹ì„ êµ¬í˜„í•˜ëŠ” ê³¼ì •ì„ ë‹¤ë£¹ë‹ˆë‹¤.

## ëª©ì°¨
1. [Karpenter ì„¤ì¹˜](#karpenter-ì„¤ì¹˜)
2. [GPU NodePool êµ¬ì„±](#gpu-nodepool-êµ¬ì„±)
3. [AI Worker Pod ë°°í¬](#ai-worker-pod-ë°°í¬)
4. [Spot Interruption Handling](#spot-interruption-handling)
5. [HPA ì„¤ì •](#hpa-ì„¤ì •)
6. [í…ŒìŠ¤íŠ¸ ë° ê²€ì¦](#í…ŒìŠ¤íŠ¸-ë°-ê²€ì¦)

---

## Karpenter ì„¤ì¹˜

### 1. IAM ì—­í•  ìƒì„±

`infra/terraform/modules/karpenter/main.tf`:
```hcl
# Karpenter Controller IAM Role
resource "aws_iam_role" "karpenter_controller" {
  name = "${var.cluster_name}-karpenter-controller"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Action = "sts:AssumeRoleWithWebIdentity"
      Effect = "Allow"
      Principal = {
        Federated = var.oidc_provider_arn
      }
      Condition = {
        StringEquals = {
          "${var.oidc_provider}:sub" = "system:serviceaccount:karpenter:karpenter"
          "${var.oidc_provider}:aud" = "sts.amazonaws.com"
        }
      }
    }]
  })
}

resource "aws_iam_policy" "karpenter_controller" {
  name = "${var.cluster_name}-karpenter-controller-policy"

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "ec2:CreateLaunchTemplate",
          "ec2:CreateFleet",
          "ec2:RunInstances",
          "ec2:CreateTags",
          "ec2:TerminateInstances",
          "ec2:DescribeInstances",
          "ec2:DescribeSecurityGroups",
          "ec2:DescribeSubnets",
          "ec2:DescribeInstanceTypes",
          "ec2:DescribeInstanceTypeOfferings",
          "ec2:DescribeAvailabilityZones",
          "ec2:DescribeLaunchTemplates",
          "ec2:DescribeSpotPriceHistory",
          "pricing:GetProducts",
          "ssm:GetParameter"
        ]
        Resource = "*"
      }
    ]
  })
}

resource "aws_iam_role_policy_attachment" "karpenter_controller" {
  role       = aws_iam_role.karpenter_controller.name
  policy_arn = aws_iam_policy.karpenter_controller.arn
}

# Node Instance Profile
resource "aws_iam_role" "karpenter_node" {
  name = "${var.cluster_name}-karpenter-node"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Action = "sts:AssumeRole"
      Effect = "Allow"
      Principal = {
        Service = "ec2.amazonaws.com"
      }
    }]
  })
}

resource "aws_iam_role_policy_attachment" "karpenter_node_policies" {
  for_each = toset([
    "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy",
    "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy",
    "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly",
    "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
  ])

  role       = aws_iam_role.karpenter_node.name
  policy_arn = each.value
}

resource "aws_iam_instance_profile" "karpenter_node" {
  name = "${var.cluster_name}-karpenter-node-profile"
  role = aws_iam_role.karpenter_node.name
}
```

**Terraform ì ìš©**:
```bash
cd infra/terraform/environments/dev
terraform apply
```

---

### 2. Helmìœ¼ë¡œ Karpenter ì„¤ì¹˜

```bash
# Helm repo ì¶”ê°€
helm repo add karpenter https://charts.karpenter.sh
helm repo update

# ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
kubectl create namespace karpenter

# Karpenter ì„¤ì¹˜
export CLUSTER_NAME=dev-pingvas-eks
export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
export KARPENTER_VERSION=v0.32.0

helm install karpenter karpenter/karpenter \
  --namespace karpenter \
  --version ${KARPENTER_VERSION} \
  --set settings.clusterName=${CLUSTER_NAME} \
  --set settings.interruptionQueueName=${CLUSTER_NAME} \
  --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"=arn:aws:iam::${AWS_ACCOUNT_ID}:role/${CLUSTER_NAME}-karpenter-controller \
  --set controller.resources.requests.cpu=1 \
  --set controller.resources.requests.memory=1Gi \
  --wait

# í™•ì¸
kubectl get pods -n karpenter
```

---

## GPU NodePool êµ¬ì„±

### 1. EC2NodeClass

`k8s/karpenter/ec2nodeclass.yaml`:
```yaml
apiVersion: karpenter.k8s.aws/v1beta1
kind: EC2NodeClass
metadata:
  name: gpu-spot
spec:
  amiFamily: AL2  # Amazon Linux 2
  role: dev-pingvas-eks-karpenter-node  # IAM Role

  subnetSelectorTerms:
    - tags:
        karpenter.sh/discovery: dev-pingvas-eks

  securityGroupSelectorTerms:
    - tags:
        karpenter.sh/discovery: dev-pingvas-eks

  # User Data (GPU ë“œë¼ì´ë²„ ì„¤ì¹˜)
  userData: |
    #!/bin/bash
    set -e

    # NVIDIA Driver ì„¤ì¹˜
    sudo yum install -y gcc kernel-devel-$(uname -r)

    distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
    curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.repo | \
      sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo

    sudo yum install -y nvidia-driver-latest-dkms
    sudo yum install -y nvidia-container-toolkit

    # NVIDIA Device Plugin ìë™ ë°œê²¬ì„ ìœ„í•œ ë ˆì´ë¸”
    echo "nvidia.com/gpu=true" >> /etc/eks/bootstrap.sh

    # Docker ì¬ì‹œì‘
    sudo systemctl restart docker

  tags:
    Name: karpenter-gpu-node
    Environment: dev
    ManagedBy: karpenter

  blockDeviceMappings:
    - deviceName: /dev/xvda
      ebs:
        volumeSize: 100Gi
        volumeType: gp3
        encrypted: true
        deleteOnTermination: true
```

---

### 2. NodePool (GPU Spot)

`k8s/karpenter/nodepool-gpu-spot.yaml`:
```yaml
apiVersion: karpenter.sh/v1beta1
kind: NodePool
metadata:
  name: gpu-spot
spec:
  template:
    metadata:
      labels:
        workload-type: gpu
        instance-lifecycle: spot

    spec:
      nodeClassRef:
        name: gpu-spot

      requirements:
        # Instance Types (GPU)
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - g4dn.xlarge    # 1x T4, 4 vCPUs, 16GB RAM
            - g4dn.2xlarge   # 1x T4, 8 vCPUs, 32GB RAM
            - g5.xlarge      # 1x A10G, 4 vCPUs, 16GB RAM
            - g5.2xlarge     # 1x A10G, 8 vCPUs, 32GB RAM

        # Capacity Type (Spot)
        - key: karpenter.sh/capacity-type
          operator: In
          values:
            - spot

        # Architecture
        - key: kubernetes.io/arch
          operator: In
          values:
            - amd64

        # Availability Zones
        - key: topology.kubernetes.io/zone
          operator: In
          values:
            - us-east-1a
            - us-east-1b
            - us-east-1c

      # Taints (GPU ì „ìš©)
      taints:
        - key: nvidia.com/gpu
          effect: NoSchedule

  # Limits
  limits:
    nvidia.com/gpu: "20"  # ìµœëŒ€ 20ê°œ GPU

  # Disruption (Scale Down)
  disruption:
    consolidationPolicy: WhenUnderutilized
    consolidateAfter: 30s  # 30ì´ˆ í›„ ìŠ¤ì¼€ì¼ ë‹¤ìš´

    # Budgets (ìŠ¤ì¼€ì¼ ë‹¤ìš´ ì œí•œ)
    budgets:
      - nodes: "10%"  # í•œ ë²ˆì— 10%ë§Œ ì œê±°
        reasons:
          - Drifted
          - Empty
```

---

### 3. NodePool (On-Demand - Enterprise Tier)

`k8s/karpenter/nodepool-gpu-ondemand.yaml`:
```yaml
apiVersion: karpenter.sh/v1beta1
kind: NodePool
metadata:
  name: gpu-ondemand-enterprise
spec:
  template:
    metadata:
      labels:
        workload-type: gpu
        instance-lifecycle: on-demand
        tier: enterprise

    spec:
      nodeClassRef:
        name: gpu-spot

      requirements:
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - g5.4xlarge   # 1x A10G, 16 vCPUs, 64GB RAM
            - g5.8xlarge   # 1x A10G, 32 vCPUs, 128GB RAM

        - key: karpenter.sh/capacity-type
          operator: In
          values:
            - on-demand

      taints:
        - key: tier
          value: enterprise
          effect: NoSchedule

  limits:
    nvidia.com/gpu: "5"

  disruption:
    consolidationPolicy: WhenEmpty
    consolidateAfter: 5m
```

**ì ìš©**:
```bash
kubectl apply -f k8s/karpenter/
```

---

## AI Worker Pod ë°°í¬

### 1. Worker Deployment

`k8s/workers/invokeai-worker.yaml`:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: invokeai-worker
  namespace: dev
spec:
  replicas: 0  # Karpenterê°€ ìë™ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§
  selector:
    matchLabels:
      app: invokeai-worker
  template:
    metadata:
      labels:
        app: invokeai-worker
        workload-type: gpu
    spec:
      # GPU ë…¸ë“œ íƒ€ê²ŸíŒ…
      nodeSelector:
        workload-type: gpu

      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule

      # Pod Anti-Affinity (ê°™ì€ ë…¸ë“œì— ì¤‘ë³µ ë°°ì¹˜ ë°©ì§€)
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - invokeai-worker
                topologyKey: kubernetes.io/hostname

      containers:
        - name: worker
          image: YOUR_ECR_REPO/invokeai-worker:latest

          resources:
            requests:
              nvidia.com/gpu: 1
              memory: 16Gi
              cpu: 4
            limits:
              nvidia.com/gpu: 1
              memory: 16Gi

          env:
            - name: REDIS_URL
              value: "redis://redis-primary:6379/0"

            - name: DATABASE_URL
              valueFrom:
                secretKeyRef:
                  name: db-credentials
                  key: url

            - name: AWS_REGION
              value: "us-east-1"

          volumeMounts:
            - name: models-efs
              mountPath: /models
              readOnly: true

            - name: shm
              mountPath: /dev/shm

      volumes:
        - name: models-efs
          persistentVolumeClaim:
            claimName: efs-models-pvc

        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 8Gi
```

---

### 2. Celery Worker ì½”ë“œ

`services/worker/worker.py`:
```python
from celery import Celery
from datetime import datetime
import torch
from diffusers import StableDiffusionPipeline

# Celery ì•±
celery_app = Celery(
    "invokeai-worker",
    broker="redis://redis-primary:6379/0",
    backend="redis://redis-primary:6379/0"
)

# GPU í™•ì¸
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# ëª¨ë¸ ë¡œë“œ (EFSì—ì„œ)
pipeline = StableDiffusionPipeline.from_pretrained(
    "/models/stable-diffusion-v1-5",
    torch_dtype=torch.float16
).to(device)

@celery_app.task(bind=True, name="tasks.generate_image")
def generate_image(self, job_id: str):
    """
    ì´ë¯¸ì§€ ìƒì„± íƒœìŠ¤í¬
    """
    from sqlalchemy.orm import Session
    from app.database import SessionLocal
    from app.models import GenerationJob

    db = SessionLocal()

    try:
        # ì‘ì—… ì¡°íšŒ
        job = db.query(GenerationJob).filter(GenerationJob.id == job_id).first()

        if not job:
            raise ValueError(f"Job {job_id} not found")

        # ì‘ì—… ì‹œì‘
        job.status = "in_progress"
        job.started_at = datetime.utcnow()
        db.commit()

        # ì´ë¯¸ì§€ ìƒì„±
        with torch.inference_mode():
            output = pipeline(
                prompt=job.prompt,
                negative_prompt=job.negative_prompt,
                width=job.width,
                height=job.height,
                num_inference_steps=job.steps,
                guidance_scale=job.cfg_scale,
                generator=torch.Generator(device).manual_seed(job.seed) if job.seed else None
            )

        image = output.images[0]

        # S3 ì—…ë¡œë“œ
        s3_key = upload_to_s3(image, job_id)

        # ì‘ì—… ì™„ë£Œ
        job.status = "completed"
        job.completed_at = datetime.utcnow()
        job.image_url = f"https://cdn.pingvas.studio/{s3_key}"

        # í¬ë ˆë”§ ì°¨ê°
        duration_seconds = int((job.completed_at - job.started_at).total_seconds())
        deduct_credits(job.user_id, duration_seconds, job_id)

        db.commit()

        return {"success": True, "image_url": job.image_url}

    except Exception as e:
        job.status = "failed"
        job.error_message = str(e)
        job.completed_at = datetime.utcnow()
        db.commit()
        raise

    finally:
        db.close()

if __name__ == "__main__":
    celery_app.worker_main([
        "worker",
        "--loglevel=info",
        "--concurrency=1",  # GPU ë©”ëª¨ë¦¬ ì œì•½
        "--pool=solo"  # Single process
    ])
```

**Dockerfile**:
```dockerfile
FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04

# Python ì„¤ì¹˜
RUN apt-get update && apt-get install -y python3.11 python3-pip

# ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip3 install -r requirements.txt

# ì•± ì½”ë“œ
COPY . /app
WORKDIR /app

CMD ["python3", "worker.py"]
```

---

## Spot Interruption Handling

### 1. AWS Node Termination Handler

```bash
helm repo add eks https://aws.github.io/eks-charts
helm repo update

helm install aws-node-termination-handler \
  eks/aws-node-termination-handler \
  --namespace kube-system \
  --set enableSpotInterruptionDraining=true \
  --set enableRebalanceMonitoring=true \
  --set enableScheduledEventDraining=true \
  --set nodeSelector.lifecycle=spot
```

---

### 2. Pod Disruption Budget

`k8s/workers/pdb.yaml`:
```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: invokeai-worker-pdb
  namespace: dev
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: invokeai-worker
```

---

## HPA ì„¤ì •

### 1. Metrics Server ì„¤ì¹˜

```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

---

### 2. KEDA (Kubernetes Event Driven Autoscaling)

```bash
helm repo add kedacore https://kedacore.github.io/charts
helm repo update

helm install keda kedacore/keda --namespace keda --create-namespace
```

**ScaledObject (Redis Queue ê¸°ë°˜)**:

`k8s/workers/scaledobject.yaml`:
```yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: invokeai-worker-scaler
  namespace: dev
spec:
  scaleTargetRef:
    name: invokeai-worker

  minReplicaCount: 0   # Scale to Zero
  maxReplicaCount: 20  # ìµœëŒ€ 20ê°œ Pod

  triggers:
    # Redis Queue Length
    - type: redis
      metadata:
        address: redis-primary:6379
        listName: celery  # Celery ê¸°ë³¸ í
        listLength: "5"   # íì— 5ê°œ ì´ìƒ ì‘ì—…ì´ ìˆìœ¼ë©´ ìŠ¤ì¼€ì¼ ì—…
        databaseIndex: "0"
```

**ì ìš©**:
```bash
kubectl apply -f k8s/workers/scaledobject.yaml
```

---

## í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### 1. Scale-to-Zero í…ŒìŠ¤íŠ¸

```bash
# íì— ì‘ì—… ì¶”ê°€
python test_enqueue.py

# Pod ìë™ ìƒì„± í™•ì¸
watch kubectl get pods -n dev

# GPU ë…¸ë“œ ìë™ ìƒì„± í™•ì¸
watch kubectl get nodes --selector=workload-type=gpu

# Karpenter ë¡œê·¸ í™•ì¸
kubectl logs -n karpenter -l app.kubernetes.io/name=karpenter -f
```

---

### 2. Spot Interruption ì‹œë®¬ë ˆì´ì…˜

```bash
# ë…¸ë“œ drain ì‹œë®¬ë ˆì´ì…˜
kubectl drain <NODE_NAME> --ignore-daemonsets --delete-emptydir-data

# Pod ì¬ìŠ¤ì¼€ì¤„ë§ í™•ì¸
kubectl get pods -n dev -w
```

---

### 3. ë¶€í•˜ í…ŒìŠ¤íŠ¸

`test_load.py`:
```python
import requests
import concurrent.futures

API_URL = "http://api.pingvas.studio"
TOKEN = "your_jwt_token"

def create_job(i):
    response = requests.post(
        f"{API_URL}/api/v1/generation/create",
        headers={"Authorization": f"Bearer {TOKEN}"},
        json={
            "prompt": f"Test image {i}",
            "model": "sd15",
            "width": 512,
            "height": 512,
            "steps": 20
        }
    )
    return response.json()

# 100ê°œ ì‘ì—… ë™ì‹œ ìƒì„±
with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
    futures = [executor.submit(create_job, i) for i in range(100)]
    results = [f.result() for f in futures]

print(f"Created {len(results)} jobs")
```

---

### 4. ë¹„ìš© ëª¨ë‹ˆí„°ë§

```bash
# Spot ì¸ìŠ¤í„´ìŠ¤ ë¹„ìš© í™•ì¸
aws ce get-cost-and-usage \
  --time-period Start=2025-01-01,End=2025-01-31 \
  --granularity DAILY \
  --metrics UnblendedCost \
  --filter file://spot-filter.json

# Karpenter ë…¸ë“œ ì •ë³´
kubectl get nodes -l karpenter.sh/capacity-type=spot -o wide
```

---

## ë‹¤ìŒ ë‹¨ê³„

GPU ì˜¤í† ìŠ¤ì¼€ì¼ë§ êµ¬ì¶•ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì´ì œ GitOps/CI/CD íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤:

**ğŸ‘‰ [Phase 5 - GitOps/CI/CD íŒŒì´í”„ë¼ì¸](./phase-05-gitops-cicd.md)**

---

## ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Karpenter ì„¤ì¹˜
- [ ] GPU NodePool êµ¬ì„±
- [ ] AI Worker Pod ë°°í¬
- [ ] Spot Interruption Handler ì„¤ì¹˜
- [ ] KEDA ì„¤ì •
- [ ] Scale-to-Zero í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] ë¶€í•˜ í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] ë¹„ìš© ìµœì í™” í™•ì¸
