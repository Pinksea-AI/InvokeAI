# InvokeAI Kubernetes SaaS êµ¬ì¶• ê°€ì´ë“œ

> EKS ê¸°ë°˜ êµ¬ë…í˜• SaaS - ê°œë°œê³„/ìš´ì˜ê³„ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë¶„ë¦¬

## ğŸ“‹ ëª©ì°¨

1. [ì•„í‚¤í…ì²˜ ê°œìš”](#1-ì•„í‚¤í…ì²˜-ê°œìš”)
2. [EKS í´ëŸ¬ìŠ¤í„° êµ¬ì¶•](#2-eks-í´ëŸ¬ìŠ¤í„°-êµ¬ì¶•)
3. [ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì „ëµ](#3-ë„¤ì„ìŠ¤í˜ì´ìŠ¤-ì „ëµ)
4. [Aurora PostgreSQL ê³µìœ  ì „ëµ](#4-aurora-postgresql-ê³µìœ -ì „ëµ)
5. [ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬](#5-ì• í”Œë¦¬ì¼€ì´ì…˜-ë°°í¬)
6. [GPU ì›Œì»¤ êµ¬ì„±](#6-gpu-ì›Œì»¤-êµ¬ì„±)
7. [Ingress ë° ë¡œë“œë°¸ëŸ°ì‹±](#7-ingress-ë°-ë¡œë“œë°¸ëŸ°ì‹±)
8. [Auto Scaling](#8-auto-scaling)
9. [ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…](#9-ëª¨ë‹ˆí„°ë§-ë°-ë¡œê¹…)
10. [CI/CD íŒŒì´í”„ë¼ì¸](#10-cicd-íŒŒì´í”„ë¼ì¸)

---

## 1. ì•„í‚¤í…ì²˜ ê°œìš”

### 1.1 ì „ì²´ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Route 53 (DNS)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CloudFront (CDN)                                â”‚
â”‚  - ì •ì  íŒŒì¼ ìºì‹±                                             â”‚
â”‚  - SSL/TLS ì¢…ë£Œ                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         AWS Application Load Balancer                       â”‚
â”‚  (Kubernetes Ingress Controllerê°€ ìë™ ìƒì„±)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  EKS Cluster                                 â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Namespace: prod (ìš´ì˜ê³„)                    â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚    â”‚
â”‚  â”‚  â”‚ API Server   â”‚  â”‚ GPU Worker   â”‚               â”‚    â”‚
â”‚  â”‚  â”‚ (Deployment) â”‚  â”‚ (Deployment) â”‚               â”‚    â”‚
â”‚  â”‚  â”‚  Replicas: 3 â”‚  â”‚  Replicas: 2 â”‚               â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚    â”‚
â”‚  â”‚                                                     â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚    â”‚
â”‚  â”‚  â”‚   Redis      â”‚  â”‚   Celery     â”‚               â”‚    â”‚
â”‚  â”‚  â”‚ (StatefulSet)â”‚  â”‚ (Deployment) â”‚               â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Namespace: dev (ê°œë°œê³„)                     â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚    â”‚
â”‚  â”‚  â”‚ API Server   â”‚  â”‚ GPU Worker   â”‚               â”‚    â”‚
â”‚  â”‚  â”‚ (Deployment) â”‚  â”‚ (Deployment) â”‚               â”‚    â”‚
â”‚  â”‚  â”‚  Replicas: 1 â”‚  â”‚  Replicas: 1 â”‚               â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚    â”‚
â”‚  â”‚                                                     â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚    â”‚
â”‚  â”‚  â”‚   Redis      â”‚                                  â”‚    â”‚
â”‚  â”‚  â”‚ (StatefulSet)â”‚                                  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   Shared Resources (kube-system ë“±)                 â”‚    â”‚
â”‚  â”‚  - Ingress Controller                               â”‚    â”‚
â”‚  â”‚  - Metrics Server                                   â”‚    â”‚
â”‚  â”‚  - Cluster Autoscaler                               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ì™¸ë¶€ AWS ì„œë¹„ìŠ¤                                  â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Aurora DB   â”‚  â”‚     S3       â”‚  â”‚   Cognito    â”‚     â”‚
â”‚  â”‚ (PostgreSQL) â”‚  â”‚  (ì´ë¯¸ì§€)    â”‚  â”‚    (ì¸ì¦)    â”‚     â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚     â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚  â”‚ â”‚   prod   â”‚ â”‚                                          â”‚
â”‚  â”‚ â”‚  schema  â”‚ â”‚                                          â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                                          â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                                          â”‚
â”‚  â”‚ â”‚   dev    â”‚ â”‚                                          â”‚
â”‚  â”‚ â”‚  schema  â”‚ â”‚                                          â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 ECS vs Kubernetes ë¹„êµ

| í•­ëª© | ECS (ì´ì „) | Kubernetes (í˜„ì¬) |
|-----|-----------|------------------|
| **ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜** | ECS Tasks/Services | Deployments/StatefulSets |
| **ë¡œë“œë°¸ëŸ°ì‹±** | ALB ìˆ˜ë™ ì„¤ì • | Ingress (ALB ìë™ ìƒì„±) |
| **ìŠ¤ì¼€ì¼ë§** | ASG + ECS Service | HPA + Cluster Autoscaler |
| **ì„¤ì • ê´€ë¦¬** | ECS Task Definition | ConfigMap + Secret |
| **ë¡œê¹…** | CloudWatch Logs | FluentBit â†’ CloudWatch |
| **ëª¨ë‹ˆí„°ë§** | CloudWatch | Prometheus + Grafana |
| **ë°°í¬ ì „ëµ** | Blue-Green (ìˆ˜ë™) | Rolling Update (ìë™) |
| **ë©€í‹° í™˜ê²½** | ë³„ë„ í´ëŸ¬ìŠ¤í„° | Namespace ë¶„ë¦¬ |

---

## 2. EKS í´ëŸ¬ìŠ¤í„° êµ¬ì¶•

### 2.1 Terraformìœ¼ë¡œ EKS ìƒì„±

```hcl
# terraform/eks.tf

# EKS ëª¨ë“ˆ ì‚¬ìš© (ê³µì‹ ëª¨ë“ˆ)
module "eks" {
  source  = "terraform-aws-modules/eks/aws"
  version = "~> 19.0"

  cluster_name    = "invokeai-cluster"
  cluster_version = "1.28"

  # VPC ì„¤ì •
  vpc_id     = aws_vpc.main.id
  subnet_ids = [
    aws_subnet.private_app_a.id,
    aws_subnet.private_app_b.id,
    aws_subnet.private_app_c.id,
  ]

  # í´ëŸ¬ìŠ¤í„° ì—”ë“œí¬ì¸íŠ¸ ì ‘ê·¼
  cluster_endpoint_public_access  = true
  cluster_endpoint_private_access = true

  # OIDC Provider (IRSAìš©)
  enable_irsa = true

  # CloudWatch ë¡œê¹…
  cluster_enabled_log_types = [
    "api",
    "audit",
    "authenticator",
    "controllerManager",
    "scheduler",
  ]

  # ë…¸ë“œ ê·¸ë£¹ ì •ì˜
  eks_managed_node_groups = {
    # ì¼ë°˜ ì›Œí¬ë¡œë“œ ë…¸ë“œ (API ì„œë²„, Redis ë“±)
    general = {
      name = "general-nodes"

      instance_types = ["t3.large"]
      capacity_type  = "ON_DEMAND"

      min_size     = 2
      max_size     = 10
      desired_size = 3

      labels = {
        role = "general"
      }

      taints = []

      # EBS CSI Driverìš© IAM ì •ì±…
      iam_role_additional_policies = {
        AmazonEBSCSIDriverPolicy = "arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy"
      }
    }

    # GPU ì›Œí¬ë¡œë“œ ë…¸ë“œ
    gpu = {
      name = "gpu-nodes"

      # GPU ì¸ìŠ¤í„´ìŠ¤
      instance_types = ["g5.xlarge"]
      ami_type       = "AL2_x86_64_GPU"  # GPU AMI
      capacity_type  = "SPOT"             # Spot ì¸ìŠ¤í„´ìŠ¤ë¡œ ë¹„ìš© ì ˆê°

      min_size     = 0
      max_size     = 10
      desired_size = 1

      labels = {
        role        = "gpu-worker"
        gpu         = "true"
        nvidia.com/gpu = "true"
      }

      taints = [
        {
          key    = "nvidia.com/gpu"
          value  = "true"
          effect = "NoSchedule"
        }
      ]

      # GPU ì¸ìŠ¤í„´ìŠ¤ì— ì í•©í•œ ì„¤ì •
      block_device_mappings = {
        xvda = {
          device_name = "/dev/xvda"
          ebs = {
            volume_size = 200  # ëª¨ë¸ ì €ì¥ìš©
            volume_type = "gp3"
            encrypted   = true
          }
        }
      }
    }
  }

  # AWS Load Balancer Controllerìš© IAM ì—­í• 
  enable_cluster_creator_admin_permissions = true

  tags = {
    Environment = "production"
    Terraform   = "true"
  }
}

# OIDC Provider (IRSA - IAM Roles for Service Accounts)
data "tls_certificate" "eks" {
  url = module.eks.cluster_oidc_issuer_url
}

resource "aws_iam_openid_connect_provider" "eks" {
  client_id_list  = ["sts.amazonaws.com"]
  thumbprint_list = [data.tls_certificate.eks.certificates[0].sha1_fingerprint]
  url             = module.eks.cluster_oidc_issuer_url
}
```

### 2.2 í•„ìˆ˜ ì• ë“œì˜¨ ì„¤ì¹˜

```hcl
# terraform/eks_addons.tf

# EBS CSI Driver (Persistent Volumeìš©)
resource "aws_eks_addon" "ebs_csi" {
  cluster_name = module.eks.cluster_name
  addon_name   = "aws-ebs-csi-driver"
  addon_version = "v1.25.0-eksbuild.1"
}

# VPC CNI (ë„¤íŠ¸ì›Œí‚¹)
resource "aws_eks_addon" "vpc_cni" {
  cluster_name = module.eks.cluster_name
  addon_name   = "vpc-cni"
  addon_version = "v1.15.0-eksbuild.2"
}

# kube-proxy
resource "aws_eks_addon" "kube_proxy" {
  cluster_name = module.eks.cluster_name
  addon_name   = "kube-proxy"
  addon_version = "v1.28.2-eksbuild.2"
}

# CoreDNS
resource "aws_eks_addon" "coredns" {
  cluster_name = module.eks.cluster_name
  addon_name   = "coredns"
  addon_version = "v1.10.1-eksbuild.6"
}
```

### 2.3 kubectl ì„¤ì •

```bash
# kubeconfig ì—…ë°ì´íŠ¸
aws eks update-kubeconfig \
  --region us-east-1 \
  --name invokeai-cluster

# í´ëŸ¬ìŠ¤í„° í™•ì¸
kubectl cluster-info
kubectl get nodes

# ì¶œë ¥ ì˜ˆì‹œ:
# NAME                          STATUS   ROLES    AGE   VERSION
# ip-10-0-11-123.ec2.internal   Ready    <none>   5m    v1.28.2-eks-...
# ip-10-0-12-124.ec2.internal   Ready    <none>   5m    v1.28.2-eks-...
```

---

## 3. ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì „ëµ

### 3.1 ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±

```yaml
# k8s/namespaces/prod.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: prod
  labels:
    name: prod
    environment: production
---
apiVersion: v1
kind: Namespace
metadata:
  name: dev
  labels:
    name: dev
    environment: development
```

```bash
kubectl apply -f k8s/namespaces/
```

### 3.2 ResourceQuota (ë¦¬ì†ŒìŠ¤ ì œí•œ)

```yaml
# k8s/namespaces/dev-quota.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: dev-quota
  namespace: dev
spec:
  hard:
    # CPU/ë©”ëª¨ë¦¬ ì œí•œ
    requests.cpu: "10"        # ìµœëŒ€ 10 CPU ìš”ì²­
    requests.memory: 20Gi     # ìµœëŒ€ 20GB ë©”ëª¨ë¦¬ ìš”ì²­
    limits.cpu: "20"          # ìµœëŒ€ 20 CPU ì œí•œ
    limits.memory: 40Gi       # ìµœëŒ€ 40GB ë©”ëª¨ë¦¬ ì œí•œ

    # GPU ì œí•œ
    requests.nvidia.com/gpu: "2"  # ìµœëŒ€ 2 GPU

    # Pod ìˆ˜ ì œí•œ
    pods: "50"                # ìµœëŒ€ 50ê°œ Pod

    # PVC ì œí•œ
    persistentvolumeclaims: "10"
    requests.storage: 100Gi
---
# k8s/namespaces/prod-quota.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: prod-quota
  namespace: prod
spec:
  hard:
    requests.cpu: "50"
    requests.memory: 100Gi
    limits.cpu: "100"
    limits.memory: 200Gi
    requests.nvidia.com/gpu: "10"
    pods: "200"
    persistentvolumeclaims: "50"
    requests.storage: 500Gi
```

### 3.3 NetworkPolicy (ë„¤íŠ¸ì›Œí¬ ê²©ë¦¬)

```yaml
# k8s/namespaces/dev-network-policy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: dev-isolation
  namespace: dev
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress

  ingress:
    # dev ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë‚´ë¶€ íŠ¸ë˜í”½ í—ˆìš©
    - from:
        - namespaceSelector:
            matchLabels:
              name: dev

    # Ingress Controllerì—ì„œì˜ íŠ¸ë˜í”½ í—ˆìš©
    - from:
        - namespaceSelector:
            matchLabels:
              name: ingress-nginx

  egress:
    # DNS í—ˆìš©
    - to:
        - namespaceSelector:
            matchLabels:
              name: kube-system
      ports:
        - protocol: UDP
          port: 53

    # ì™¸ë¶€ AWS ì„œë¹„ìŠ¤ í—ˆìš© (Aurora, S3 ë“±)
    - to:
        - namespaceSelector: {}
      ports:
        - protocol: TCP
          port: 443
        - protocol: TCP
          port: 5432  # PostgreSQL
        - protocol: TCP
          port: 6379  # Redis
```

---

## 4. Aurora PostgreSQL ê³µìœ  ì „ëµ

### 4.1 ìŠ¤í‚¤ë§ˆ ë¶„ë¦¬ ë°©ì‹

**ì¥ì :**
- ë‹¨ì¼ DB ì—°ê²°
- ë°ì´í„° ë¬¼ë¦¬ì  ë¶„ë¦¬
- ë°±ì—…/ë³µì› ê°„í¸

```sql
-- Aurora DBì—ì„œ ì‹¤í–‰

-- 1. ìŠ¤í‚¤ë§ˆ ìƒì„±
CREATE SCHEMA IF NOT EXISTS prod;
CREATE SCHEMA IF NOT EXISTS dev;

-- 2. ì‚¬ìš©ì ìƒì„± (ìŠ¤í‚¤ë§ˆë³„)
CREATE USER invokeai_prod WITH PASSWORD 'secure_password_prod';
CREATE USER invokeai_dev WITH PASSWORD 'secure_password_dev';

-- 3. ê¶Œí•œ ë¶€ì—¬
GRANT ALL PRIVILEGES ON SCHEMA prod TO invokeai_prod;
GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA prod TO invokeai_prod;
ALTER DEFAULT PRIVILEGES IN SCHEMA prod GRANT ALL ON TABLES TO invokeai_prod;

GRANT ALL PRIVILEGES ON SCHEMA dev TO invokeai_dev;
GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA dev TO invokeai_dev;
ALTER DEFAULT PRIVILEGES IN SCHEMA dev GRANT ALL ON TABLES TO invokeai_dev;

-- 4. Search Path ì„¤ì •
ALTER USER invokeai_prod SET search_path TO prod, public;
ALTER USER invokeai_dev SET search_path TO dev, public;

-- 5. í…Œì´ë¸” ìƒì„± (ê° ìŠ¤í‚¤ë§ˆì—)
-- prod ìŠ¤í‚¤ë§ˆ
CREATE TABLE prod.users (
    id VARCHAR(36) PRIMARY KEY,
    email VARCHAR(255) UNIQUE NOT NULL,
    -- ... (ë‚˜ë¨¸ì§€ í•„ë“œ)
);

CREATE TABLE prod.images (
    image_name VARCHAR(255) PRIMARY KEY,
    user_id VARCHAR(36) REFERENCES prod.users(id),
    -- ... (ë‚˜ë¨¸ì§€ í•„ë“œ)
);

-- dev ìŠ¤í‚¤ë§ˆ (ë™ì¼í•œ êµ¬ì¡°)
CREATE TABLE dev.users (
    id VARCHAR(36) PRIMARY KEY,
    email VARCHAR(255) UNIQUE NOT NULL,
    -- ...
);

CREATE TABLE dev.images (
    image_name VARCHAR(255) PRIMARY KEY,
    user_id VARCHAR(36) REFERENCES dev.users(id),
    -- ...
);
```

### 4.2 Terraformìœ¼ë¡œ Aurora ìƒì„±

```hcl
# terraform/aurora.tf

# Aurora Cluster (PostgreSQL í˜¸í™˜)
resource "aws_rds_cluster" "main" {
  cluster_identifier      = "invokeai-aurora-cluster"
  engine                  = "aurora-postgresql"
  engine_version          = "15.4"
  engine_mode             = "provisioned"

  database_name           = "invokeai"
  master_username         = "admin"
  master_password         = random_password.aurora_master.result

  # ë„¤íŠ¸ì›Œí¬
  db_subnet_group_name    = aws_db_subnet_group.main.name
  vpc_security_group_ids  = [aws_security_group.aurora.id]

  # ë°±ì—…
  backup_retention_period = 7
  preferred_backup_window = "03:00-04:00"

  # ê³ ê°€ìš©ì„±
  availability_zones = [
    "us-east-1a",
    "us-east-1b",
    "us-east-1c",
  ]

  # ìŠ¤ëƒ…ìƒ·
  skip_final_snapshot     = false
  final_snapshot_identifier = "invokeai-final-snapshot"

  # ì•”í˜¸í™”
  storage_encrypted = true
  kms_key_id        = aws_kms_key.aurora.arn

  # ì„±ëŠ¥ ê°œì„ 
  enabled_cloudwatch_logs_exports = ["postgresql"]

  # Serverless v2 (ë¹„ìš© ì ˆê°)
  serverlessv2_scaling_configuration {
    min_capacity = 0.5  # 0.5 ACU (ì•½ 1GB RAM)
    max_capacity = 16   # 16 ACU (ì•½ 32GB RAM)
  }

  tags = {
    Name = "invokeai-aurora-cluster"
  }
}

# Aurora ì¸ìŠ¤í„´ìŠ¤
resource "aws_rds_cluster_instance" "main" {
  count = 2  # Writer 1ê°œ + Reader 1ê°œ

  identifier         = "invokeai-aurora-${count.index}"
  cluster_identifier = aws_rds_cluster.main.id
  instance_class     = "db.serverless"  # Serverless v2
  engine             = aws_rds_cluster.main.engine
  engine_version     = aws_rds_cluster.main.engine_version

  # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
  performance_insights_enabled = true
  monitoring_interval          = 60
  monitoring_role_arn          = aws_iam_role.rds_monitoring.arn
}

# DB Subnet Group
resource "aws_db_subnet_group" "main" {
  name       = "invokeai-aurora-subnet-group"
  subnet_ids = [
    aws_subnet.private_data_a.id,
    aws_subnet.private_data_b.id,
    aws_subnet.private_data_c.id,
  ]
}

# ë³´ì•ˆ ê·¸ë£¹
resource "aws_security_group" "aurora" {
  name        = "aurora-sg"
  description = "Security group for Aurora cluster"
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port       = 5432
    to_port         = 5432
    protocol        = "tcp"
    security_groups = [module.eks.cluster_security_group_id]
    description     = "PostgreSQL from EKS"
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}
```

### 4.3 Kubernetes Secret (DB ì—°ê²° ì •ë³´)

```yaml
# k8s/secrets/prod-db-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: db-credentials
  namespace: prod
type: Opaque
stringData:
  DB_HOST: "invokeai-aurora-cluster.cluster-xxxxx.us-east-1.rds.amazonaws.com"
  DB_PORT: "5432"
  DB_NAME: "invokeai"
  DB_USER: "invokeai_prod"
  DB_PASSWORD: "secure_password_prod"
  DB_SCHEMA: "prod"
---
# k8s/secrets/dev-db-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: db-credentials
  namespace: dev
type: Opaque
stringData:
  DB_HOST: "invokeai-aurora-cluster.cluster-xxxxx.us-east-1.rds.amazonaws.com"
  DB_PORT: "5432"
  DB_NAME: "invokeai"
  DB_USER: "invokeai_dev"
  DB_PASSWORD: "secure_password_dev"
  DB_SCHEMA: "dev"
```

**ë˜ëŠ” AWS Secrets Manager ì‚¬ìš© (ê¶Œì¥):**

```yaml
# k8s/secrets/external-secrets.yaml
apiVersion: external-secrets.io/v1beta1
kind: SecretStore
metadata:
  name: aws-secrets-manager
  namespace: prod
spec:
  provider:
    aws:
      service: SecretsManager
      region: us-east-1
      auth:
        jwt:
          serviceAccountRef:
            name: external-secrets-sa
---
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: db-credentials
  namespace: prod
spec:
  refreshInterval: 1h
  secretStoreRef:
    name: aws-secrets-manager
    kind: SecretStore
  target:
    name: db-credentials
    creationPolicy: Owner
  data:
    - secretKey: DB_PASSWORD
      remoteRef:
        key: invokeai/prod/database
        property: password
```

### 4.4 ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ìˆ˜ì •

```python
# invokeai/app/services/config/config_default.py
from pydantic_settings import BaseSettings


class InvokeAIAppConfig(BaseSettings):
    # ... ê¸°ì¡´ ì„¤ì •

    # DB ì„¤ì •
    db_host: str = "localhost"
    db_port: int = 5432
    db_name: str = "invokeai"
    db_user: str = "invokeai"
    db_password: str = ""
    db_schema: str = "public"  # âœ… ì¶”ê°€: ìŠ¤í‚¤ë§ˆ ì§€ì •

    @property
    def database_url(self) -> str:
        """SQLAlchemy connection string"""
        # âœ… search_path í¬í•¨
        return (
            f"postgresql://{self.db_user}:{self.db_password}"
            f"@{self.db_host}:{self.db_port}/{self.db_name}"
            f"?options=-c%20search_path={self.db_schema}"
        )
```

**ë˜ëŠ” SQLAlchemyì—ì„œ ì§ì ‘ ì„¤ì •:**

```python
# invokeai/app/services/shared/database.py
from sqlalchemy import create_engine, event


def create_db_engine(config: InvokeAIAppConfig):
    """DB ì—”ì§„ ìƒì„±"""

    engine = create_engine(
        config.database_url,
        pool_size=10,
        max_overflow=20,
    )

    # ì—°ê²° ì‹œ ìŠ¤í‚¤ë§ˆ ì„¤ì •
    @event.listens_for(engine, "connect")
    def set_search_path(dbapi_conn, connection_record):
        cursor = dbapi_conn.cursor()
        cursor.execute(f"SET search_path TO {config.db_schema}, public")
        cursor.close()

    return engine
```

---

## 5. ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬

### 5.1 ConfigMap (í™˜ê²½ ì„¤ì •)

```yaml
# k8s/prod/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: invokeai-config
  namespace: prod
data:
  ENVIRONMENT: "production"
  LOG_LEVEL: "INFO"

  # S3
  S3_BUCKET: "invokeai-images-prod"
  S3_REGION: "us-east-1"

  # Redis
  REDIS_HOST: "redis-service.prod.svc.cluster.local"
  REDIS_PORT: "6379"

  # Cognito
  COGNITO_REGION: "us-east-1"
  COGNITO_USER_POOL_ID: "us-east-1_XXXXXXXXX"

  # ê¸°íƒ€
  CELERY_BROKER_URL: "redis://redis-service.prod.svc.cluster.local:6379/0"
```

### 5.2 API Server Deployment

```yaml
# k8s/prod/api-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
  namespace: prod
  labels:
    app: api-server
    version: v1
spec:
  replicas: 3
  selector:
    matchLabels:
      app: api-server

  # ë¡¤ë§ ì—…ë°ì´íŠ¸ ì „ëµ
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1        # ìµœëŒ€ 1ê°œ ì¶”ê°€ Pod
      maxUnavailable: 0  # ìµœì†Œ 3ê°œ ìœ ì§€

  template:
    metadata:
      labels:
        app: api-server
        version: v1
    spec:
      serviceAccountName: api-server-sa

      # Init Container (DB ë§ˆì´ê·¸ë ˆì´ì…˜)
      initContainers:
        - name: migrate
          image: 123456789.dkr.ecr.us-east-1.amazonaws.com/invokeai-api:latest
          command: ["alembic", "upgrade", "head"]
          envFrom:
            - configMapRef:
                name: invokeai-config
            - secretRef:
                name: db-credentials

      containers:
        - name: api
          image: 123456789.dkr.ecr.us-east-1.amazonaws.com/invokeai-api:latest

          ports:
            - containerPort: 9090
              name: http
              protocol: TCP

          # í™˜ê²½ ë³€ìˆ˜
          envFrom:
            - configMapRef:
                name: invokeai-config
            - secretRef:
                name: db-credentials

          env:
            # Pod ì •ë³´
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace

          # ë¦¬ì†ŒìŠ¤ ì œí•œ
          resources:
            requests:
              cpu: "500m"      # 0.5 CPU
              memory: "1Gi"
            limits:
              cpu: "2000m"     # 2 CPU
              memory: "4Gi"

          # Health Check
          livenessProbe:
            httpGet:
              path: /api/v1/health
              port: 9090
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3

          readinessProbe:
            httpGet:
              path: /api/v1/ready
              port: 9090
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 3

          # Graceful Shutdown
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh", "-c", "sleep 15"]
---
# k8s/prod/api-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: api-service
  namespace: prod
  labels:
    app: api-server
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 9090
      protocol: TCP
      name: http
  selector:
    app: api-server
```

### 5.3 Redis StatefulSet

```yaml
# k8s/prod/redis-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis
  namespace: prod
spec:
  serviceName: redis-service
  replicas: 1
  selector:
    matchLabels:
      app: redis

  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
        - name: redis
          image: redis:7-alpine

          ports:
            - containerPort: 6379
              name: redis

          command:
            - redis-server
            - --appendonly yes
            - --requirepass $(REDIS_PASSWORD)

          env:
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: redis-secret
                  key: password

          resources:
            requests:
              cpu: "100m"
              memory: "256Mi"
            limits:
              cpu: "500m"
              memory: "1Gi"

          volumeMounts:
            - name: redis-data
              mountPath: /data

          livenessProbe:
            exec:
              command:
                - redis-cli
                - ping
            initialDelaySeconds: 30
            periodSeconds: 10

  # PersistentVolume ìš”ì²­
  volumeClaimTemplates:
    - metadata:
        name: redis-data
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: gp3  # EBS GP3
        resources:
          requests:
            storage: 10Gi
---
apiVersion: v1
kind: Service
metadata:
  name: redis-service
  namespace: prod
spec:
  type: ClusterIP
  clusterIP: None  # Headless Service
  ports:
    - port: 6379
      targetPort: 6379
  selector:
    app: redis
```

---

## 6. GPU ì›Œì»¤ êµ¬ì„±

### 6.1 GPU Node í™•ì¸

```bash
# GPU ë…¸ë“œ í™•ì¸
kubectl get nodes -l role=gpu-worker

# GPU ë¦¬ì†ŒìŠ¤ í™•ì¸
kubectl describe node <gpu-node-name> | grep nvidia.com/gpu

# ì¶œë ¥ ì˜ˆì‹œ:
# nvidia.com/gpu:     1
```

### 6.2 NVIDIA Device Plugin ì„¤ì¹˜

```bash
# NVIDIA Device Plugin ë°°í¬
kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.14.3/nvidia-device-plugin.yml

# í™•ì¸
kubectl get pods -n kube-system | grep nvidia-device-plugin
```

### 6.3 GPU Worker Deployment

```yaml
# k8s/prod/gpu-worker-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gpu-worker
  namespace: prod
  labels:
    app: gpu-worker
spec:
  replicas: 2
  selector:
    matchLabels:
      app: gpu-worker

  template:
    metadata:
      labels:
        app: gpu-worker
    spec:
      serviceAccountName: gpu-worker-sa

      # GPU ë…¸ë“œì—ë§Œ ìŠ¤ì¼€ì¤„ë§
      nodeSelector:
        role: gpu-worker

      # Taint í—ˆìš©
      tolerations:
        - key: nvidia.com/gpu
          operator: Equal
          value: "true"
          effect: NoSchedule

      # Init Container (ëª¨ë¸ ë‹¤ìš´ë¡œë“œ)
      initContainers:
        - name: download-models
          image: 123456789.dkr.ecr.us-east-1.amazonaws.com/invokeai-worker:latest
          command: ["/bin/sh", "-c"]
          args:
            - |
              echo "Checking models..."
              if [ ! -f /models/.initialized ]; then
                echo "Downloading models from S3..."
                aws s3 sync s3://invokeai-models-bucket/models /models
                touch /models/.initialized
              fi
          volumeMounts:
            - name: models
              mountPath: /models
          envFrom:
            - configMapRef:
                name: invokeai-config

      containers:
        - name: worker
          image: 123456789.dkr.ecr.us-east-1.amazonaws.com/invokeai-worker:latest

          command: ["celery", "-A", "invokeai.app.services.celery_app", "worker"]
          args:
            - --loglevel=info
            - --concurrency=1
            - --queue=gpu_tasks
            - --max-tasks-per-child=10  # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€

          # í™˜ê²½ ë³€ìˆ˜
          envFrom:
            - configMapRef:
                name: invokeai-config
            - secretRef:
                name: db-credentials

          env:
            - name: WORKER_TYPE
              value: "gpu"
            - name: CUDA_VISIBLE_DEVICES
              value: "0"

          # GPU ë¦¬ì†ŒìŠ¤ ìš”ì²­
          resources:
            requests:
              cpu: "2000m"
              memory: "8Gi"
              nvidia.com/gpu: "1"  # GPU 1ê°œ ìš”ì²­
            limits:
              cpu: "4000m"
              memory: "16Gi"
              nvidia.com/gpu: "1"  # GPU 1ê°œ ì œí•œ

          # Health Check (GPU ì‘ì—…ì€ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ)
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - "celery -A invokeai.app.services.celery_app inspect ping -d celery@$HOSTNAME"
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 5

          # ë³¼ë¥¨ ë§ˆìš´íŠ¸
          volumeMounts:
            - name: models
              mountPath: /models
            - name: shm
              mountPath: /dev/shm  # ê³µìœ  ë©”ëª¨ë¦¬ (PyTorch)

      # ë³¼ë¥¨
      volumes:
        - name: models
          persistentVolumeClaim:
            claimName: models-pvc
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 4Gi
---
# k8s/prod/models-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: models-pvc
  namespace: prod
spec:
  accessModes:
    - ReadWriteMany  # ì—¬ëŸ¬ Podì—ì„œ ê³µìœ 
  storageClassName: efs  # EFS ì‚¬ìš© (ëª¨ë¸ ê³µìœ ìš©)
  resources:
    requests:
      storage: 100Gi
```

### 6.4 EFS ì„¤ì • (ëª¨ë¸ ê³µìœ ìš©)

**Terraformìœ¼ë¡œ EFS ìƒì„±:**

```hcl
# terraform/efs.tf

# EFS íŒŒì¼ ì‹œìŠ¤í…œ
resource "aws_efs_file_system" "models" {
  creation_token = "invokeai-models"
  encrypted      = true

  performance_mode = "generalPurpose"
  throughput_mode  = "bursting"

  lifecycle_policy {
    transition_to_ia = "AFTER_30_DAYS"  # Infrequent Accessë¡œ ì´ë™
  }

  tags = {
    Name = "invokeai-models"
  }
}

# EFS ë§ˆìš´íŠ¸ íƒ€ê²Ÿ (ê° AZ)
resource "aws_efs_mount_target" "models" {
  count = 3

  file_system_id  = aws_efs_file_system.models.id
  subnet_id       = [
    aws_subnet.private_app_a.id,
    aws_subnet.private_app_b.id,
    aws_subnet.private_app_c.id,
  ][count.index]
  security_groups = [aws_security_group.efs.id]
}

# EFS ë³´ì•ˆ ê·¸ë£¹
resource "aws_security_group" "efs" {
  name        = "efs-sg"
  description = "Security group for EFS"
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port       = 2049
    to_port         = 2049
    protocol        = "tcp"
    security_groups = [module.eks.cluster_security_group_id]
    description     = "NFS from EKS"
  }
}
```

**EFS CSI Driver ì„¤ì¹˜:**

```bash
# Helmìœ¼ë¡œ EFS CSI Driver ì„¤ì¹˜
helm repo add aws-efs-csi-driver https://kubernetes-sigs.github.io/aws-efs-csi-driver/
helm repo update

helm install aws-efs-csi-driver aws-efs-csi-driver/aws-efs-csi-driver \
  --namespace kube-system \
  --set controller.serviceAccount.create=true \
  --set controller.serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="arn:aws:iam::123456789:role/EFSCSIDriverRole"
```

**StorageClass ìƒì„±:**

```yaml
# k8s/storage/efs-storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: efs
provisioner: efs.csi.aws.com
parameters:
  provisioningMode: efs-ap
  fileSystemId: fs-xxxxxxxxx  # EFS ID
  directoryPerms: "700"
```

### 6.5 ìš°ì„ ìˆœìœ„ í ì„¤ì •

```python
# invokeai/app/services/celery_app.py
from celery import Celery
from kombu import Queue, Exchange

app = Celery("invokeai")

# ìš°ì„ ìˆœìœ„ í ì„¤ì •
app.conf.task_queues = [
    # ê³ ìš°ì„ ìˆœìœ„: Enterprise ì‚¬ìš©ì
    Queue(
        "gpu_tasks_high",
        Exchange("gpu_tasks_high"),
        routing_key="gpu.high",
        priority=10,
    ),
    # ì¤‘ê°„ ìš°ì„ ìˆœìœ„: Pro ì‚¬ìš©ì
    Queue(
        "gpu_tasks_medium",
        Exchange("gpu_tasks_medium"),
        routing_key="gpu.medium",
        priority=5,
    ),
    # ë‚®ì€ ìš°ì„ ìˆœìœ„: Free ì‚¬ìš©ì
    Queue(
        "gpu_tasks_low",
        Exchange("gpu_tasks_low"),
        routing_key="gpu.low",
        priority=1,
    ),
]

app.conf.task_default_priority = 5
```

**ì‚¬ìš©ì í”Œëœì— ë”°ë¥¸ í ì„ íƒ:**

```python
# invokeai/app/api/routers/images.py
from invokeai.app.services.celery_app import app as celery_app
from invokeai.app.services.subscription import get_user_plan

@router.post("/generate")
async def generate_image(
    prompt: str,
    user_id: str = Depends(get_current_user_id),
):
    # ì‚¬ìš©ì í”Œëœ í™•ì¸
    plan = await get_user_plan(user_id)

    # í”Œëœì— ë”°ë¥¸ í ì„ íƒ
    queue_map = {
        "enterprise": "gpu_tasks_high",
        "pro": "gpu_tasks_medium",
        "free": "gpu_tasks_low",
    }

    queue = queue_map.get(plan, "gpu_tasks_low")

    # Celery ì‘ì—… ì „ì†¡
    task = celery_app.send_task(
        "invokeai.tasks.generate_image",
        args=[user_id, prompt],
        queue=queue,
    )

    return {"task_id": task.id}
```

---

## 7. Ingress ë° ë¡œë“œë°¸ëŸ°ì‹±

### 7.1 AWS Load Balancer Controller ì„¤ì¹˜

```bash
# IAM Policy ìƒì„± (Terraformìœ¼ë¡œ ë¯¸ë¦¬ ìƒì„±)
# terraform/lb_controller.tf

# Helmìœ¼ë¡œ ì„¤ì¹˜
helm repo add eks https://aws.github.io/eks-charts
helm repo update

helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
  -n kube-system \
  --set clusterName=invokeai-cluster \
  --set serviceAccount.create=true \
  --set serviceAccount.name=aws-load-balancer-controller \
  --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="arn:aws:iam::123456789:role/AmazonEKSLoadBalancerControllerRole"

# í™•ì¸
kubectl get pods -n kube-system | grep aws-load-balancer-controller
```

### 7.2 Ingress (ìš´ì˜ê³„)

```yaml
# k8s/prod/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: api-ingress
  namespace: prod
  annotations:
    # ALB ì„¤ì •
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip

    # SSL/TLS
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-1:123456789:certificate/xxxxx
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}, {"HTTPS":443}]'
    alb.ingress.kubernetes.io/ssl-redirect: '443'

    # Health Check
    alb.ingress.kubernetes.io/healthcheck-path: /api/v1/health
    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '15'
    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '5'
    alb.ingress.kubernetes.io/healthy-threshold-count: '2'
    alb.ingress.kubernetes.io/unhealthy-threshold-count: '2'

    # ì„±ëŠ¥
    alb.ingress.kubernetes.io/load-balancer-attributes: idle_timeout.timeout_seconds=60

    # WAF (ì„ íƒì‚¬í•­)
    alb.ingress.kubernetes.io/wafv2-acl-arn: arn:aws:wafv2:us-east-1:123456789:regional/webacl/invokeai-waf/xxxxx

    # íƒœê·¸
    alb.ingress.kubernetes.io/tags: Environment=production,Application=invokeai

spec:
  rules:
    - host: api.invokeai.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: api-service
                port:
                  number: 80
```

### 7.3 Ingress (ê°œë°œê³„)

```yaml
# k8s/dev/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: api-ingress
  namespace: dev
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-1:123456789:certificate/xxxxx
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}, {"HTTPS":443}]'
    alb.ingress.kubernetes.io/ssl-redirect: '443'

    # ê°œë°œê³„ëŠ” Basic Auth ì¶”ê°€ (ë³´ì•ˆ)
    alb.ingress.kubernetes.io/auth-type: cognito
    alb.ingress.kubernetes.io/auth-idp-cognito: '{"UserPoolArn":"arn:aws:cognito-idp:us-east-1:123456789:userpool/xxxxx","UserPoolClientId":"xxxxx","UserPoolDomain":"invokeai-dev"}'

spec:
  rules:
    - host: dev-api.invokeai.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: api-service
                port:
                  number: 80
```

### 7.4 CloudFront ì—°ë™ (ì„ íƒì‚¬í•­)

```hcl
# terraform/cloudfront.tf

# CloudFront Distribution
resource "aws_cloudfront_distribution" "api" {
  enabled = true
  comment = "InvokeAI API Distribution"

  # ALBë¥¼ Originìœ¼ë¡œ
  origin {
    domain_name = aws_lb.main.dns_name  # ALB DNS
    origin_id   = "alb"

    custom_origin_config {
      http_port              = 80
      https_port             = 443
      origin_protocol_policy = "https-only"
      origin_ssl_protocols   = ["TLSv1.2"]
    }
  }

  # ê¸°ë³¸ ìºì‹œ ë™ì‘ (APIëŠ” ìºì‹± ì•ˆ í•¨)
  default_cache_behavior {
    target_origin_id       = "alb"
    viewer_protocol_policy = "redirect-to-https"
    allowed_methods        = ["DELETE", "GET", "HEAD", "OPTIONS", "PATCH", "POST", "PUT"]
    cached_methods         = ["GET", "HEAD"]

    forwarded_values {
      query_string = true
      headers      = ["Authorization", "Host"]

      cookies {
        forward = "all"
      }
    }

    min_ttl     = 0
    default_ttl = 0
    max_ttl     = 0
  }

  # ì •ì  íŒŒì¼ ìºì‹± (ì´ë¯¸ì§€ ë“±)
  ordered_cache_behavior {
    path_pattern           = "/static/*"
    target_origin_id       = "alb"
    viewer_protocol_policy = "redirect-to-https"
    allowed_methods        = ["GET", "HEAD"]
    cached_methods         = ["GET", "HEAD"]

    forwarded_values {
      query_string = false
      cookies {
        forward = "none"
      }
    }

    min_ttl     = 0
    default_ttl = 86400   # 1ì¼
    max_ttl     = 31536000  # 1ë…„
  }

  # SSL/TLS
  viewer_certificate {
    acm_certificate_arn      = aws_acm_certificate.main.arn
    ssl_support_method       = "sni-only"
    minimum_protocol_version = "TLSv1.2_2021"
  }

  # Geo Restriction (ì„ íƒì‚¬í•­)
  restrictions {
    geo_restriction {
      restriction_type = "none"
    }
  }
}
```

---

## 8. Auto Scaling

### 8.1 Horizontal Pod Autoscaler (HPA)

**Metrics Server ì„¤ì¹˜:**

```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# í™•ì¸
kubectl get deployment metrics-server -n kube-system
```

**API Server HPA:**

```yaml
# k8s/prod/api-hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-server-hpa
  namespace: prod
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-server

  minReplicas: 3
  maxReplicas: 20

  metrics:
    # CPU ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70

    # ë©”ëª¨ë¦¬ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80

  # ìŠ¤ì¼€ì¼ë§ ë™ì‘
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # 5ë¶„ ëŒ€ê¸°
      policies:
        - type: Percent
          value: 50  # í•œ ë²ˆì— 50%ë§Œ ì¶•ì†Œ
          periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0  # ì¦‰ì‹œ í™•ì¥
      policies:
        - type: Percent
          value: 100  # í•œ ë²ˆì— 2ë°° í™•ì¥ ê°€ëŠ¥
          periodSeconds: 15
        - type: Pods
          value: 4  # ìµœëŒ€ 4ê°œì”© ì¶”ê°€
          periodSeconds: 15
      selectPolicy: Max  # ë” í° ê°’ ì„ íƒ
```

**GPU Worker HPA:**

```yaml
# k8s/prod/gpu-worker-hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: gpu-worker-hpa
  namespace: prod
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: gpu-worker

  minReplicas: 1
  maxReplicas: 10

  metrics:
    # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  (Custom Metric)
    - type: Pods
      pods:
        metric:
          name: gpu_memory_utilization
        target:
          type: AverageValue
          averageValue: "80"

    # Queue ê¸¸ì´ ê¸°ë°˜ (Custom Metric)
    - type: External
      external:
        metric:
          name: redis_queue_length
          selector:
            matchLabels:
              queue: gpu_tasks
        target:
          type: AverageValue
          averageValue: "5"  # Queueì— 5ê°œ ì´ìƒì´ë©´ ìŠ¤ì¼€ì¼ì—…

  behavior:
    scaleDown:
      stabilizationWindowSeconds: 600  # 10ë¶„ ëŒ€ê¸° (GPU ë¹„ìŒˆ)
      policies:
        - type: Pods
          value: 1  # í•œ ë²ˆì— 1ê°œì”©ë§Œ ì¶•ì†Œ
          periodSeconds: 300  # 5ë¶„ë§ˆë‹¤
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Pods
          value: 2  # í•œ ë²ˆì— 2ê°œì”© ì¶”ê°€
          periodSeconds: 60
```

### 8.2 Cluster Autoscaler

```bash
# Helmìœ¼ë¡œ Cluster Autoscaler ì„¤ì¹˜
helm repo add autoscaler https://kubernetes.github.io/autoscaler
helm repo update

helm install cluster-autoscaler autoscaler/cluster-autoscaler \
  --namespace kube-system \
  --set autoDiscovery.clusterName=invokeai-cluster \
  --set awsRegion=us-east-1 \
  --set rbac.serviceAccount.create=true \
  --set rbac.serviceAccount.name=cluster-autoscaler \
  --set rbac.serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="arn:aws:iam::123456789:role/ClusterAutoscalerRole" \
  --set extraArgs.balance-similar-node-groups=true \
  --set extraArgs.skip-nodes-with-system-pods=false
```

**Cluster Autoscaler ì„¤ì •:**

```yaml
# k8s/cluster-autoscaler-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-autoscaler-priority-expander
  namespace: kube-system
data:
  priorities: |
    10:
      - .*-general-.*
    50:
      - .*-gpu-.*
```

### 8.3 Karpenter (ëŒ€ì•ˆ - ë” ë¹ ë¥¸ ìŠ¤ì¼€ì¼ë§)

```bash
# Karpenter ì„¤ì¹˜ (Cluster Autoscaler ëŒ€ì‹ )
helm upgrade --install karpenter oci://public.ecr.aws/karpenter/karpenter \
  --version v0.32.0 \
  --namespace karpenter \
  --create-namespace \
  --set settings.aws.clusterName=invokeai-cluster \
  --set settings.aws.defaultInstanceProfile=KarpenterNodeInstanceProfile \
  --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="arn:aws:iam::123456789:role/KarpenterControllerRole"
```

**Karpenter Provisioner:**

```yaml
# k8s/karpenter/provisioner.yaml
apiVersion: karpenter.sh/v1alpha5
kind: Provisioner
metadata:
  name: default
spec:
  # ì œì•½ì¡°ê±´
  requirements:
    - key: karpenter.sh/capacity-type
      operator: In
      values: ["spot", "on-demand"]
    - key: kubernetes.io/arch
      operator: In
      values: ["amd64"]
    - key: node.kubernetes.io/instance-type
      operator: In
      values: ["t3.large", "t3.xlarge", "t3.2xlarge"]

  limits:
    resources:
      cpu: 100
      memory: 200Gi

  providerRef:
    name: default

  # TTL ì„¤ì •
  ttlSecondsAfterEmpty: 300  # ë¹ˆ ë…¸ë“œ 5ë¶„ í›„ ì œê±°
  ttlSecondsUntilExpired: 86400  # 24ì‹œê°„ í›„ êµì²´
---
# GPU Provisioner
apiVersion: karpenter.sh/v1alpha5
kind: Provisioner
metadata:
  name: gpu
spec:
  requirements:
    - key: karpenter.sh/capacity-type
      operator: In
      values: ["spot"]  # Spotë§Œ ì‚¬ìš©
    - key: node.kubernetes.io/instance-type
      operator: In
      values: ["g5.xlarge", "g5.2xlarge"]
    - key: nvidia.com/gpu
      operator: Exists

  taints:
    - key: nvidia.com/gpu
      value: "true"
      effect: NoSchedule

  limits:
    resources:
      nvidia.com/gpu: "10"

  ttlSecondsAfterEmpty: 600  # GPUëŠ” 10ë¶„ ëŒ€ê¸°
```

---

## 9. ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

### 9.1 Prometheus + Grafana ì„¤ì¹˜

```bash
# kube-prometheus-stack ì„¤ì¹˜
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace \
  --set prometheus.prometheusSpec.retention=30d \
  --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi \
  --set grafana.adminPassword=admin123 \
  --set grafana.ingress.enabled=true \
  --set grafana.ingress.hosts[0]=grafana.invokeai.com
```

### 9.2 ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

**ServiceMonitor (API Server):**

```yaml
# k8s/monitoring/api-servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: api-server
  namespace: prod
  labels:
    app: api-server
spec:
  selector:
    matchLabels:
      app: api-server
  endpoints:
    - port: http
      path: /metrics
      interval: 30s
```

**Prometheus ë£°:**

```yaml
# k8s/monitoring/prometheus-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: invokeai-alerts
  namespace: monitoring
spec:
  groups:
    - name: invokeai
      interval: 30s
      rules:
        # API ì„œë²„ ë‹¤ìš´
        - alert: APIServerDown
          expr: up{job="api-server"} == 0
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: "API Server is down"
            description: "API Server {{ $labels.pod }} has been down for 1 minute"

        # GPU ì›Œì»¤ ë‹¤ìš´
        - alert: GPUWorkerDown
          expr: up{job="gpu-worker"} == 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "GPU Worker is down"

        # High CPU
        - alert: HighCPUUsage
          expr: rate(container_cpu_usage_seconds_total{namespace="prod"}[5m]) > 0.8
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High CPU usage detected"

        # High Memory
        - alert: HighMemoryUsage
          expr: container_memory_usage_bytes{namespace="prod"} / container_spec_memory_limit_bytes{namespace="prod"} > 0.9
          for: 5m
          labels:
            severity: warning

        # Queue ë°±ë¡œê·¸
        - alert: HighQueueBacklog
          expr: redis_queue_length{queue="gpu_tasks"} > 100
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "High queue backlog detected"
```

### 9.3 ë¡œê¹… (FluentBit â†’ CloudWatch)

```bash
# AWS for FluentBit ì„¤ì¹˜
helm repo add aws https://aws.github.io/eks-charts
helm repo update

helm install aws-for-fluent-bit aws/aws-for-fluent-bit \
  --namespace kube-system \
  --set serviceAccount.create=true \
  --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="arn:aws:iam::123456789:role/FluentBitRole" \
  --set cloudWatch.enabled=true \
  --set cloudWatch.region=us-east-1 \
  --set cloudWatch.logGroupName=/aws/eks/invokeai-cluster/logs
```

**FluentBit ì»¤ìŠ¤í…€ ì„¤ì •:**

```yaml
# k8s/logging/fluentbit-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-bit-config
  namespace: kube-system
data:
  fluent-bit.conf: |
    [SERVICE]
        Flush         5
        Log_Level     info
        Daemon        off

    [INPUT]
        Name              tail
        Path              /var/log/containers/*_prod_*.log
        Parser            docker
        Tag               prod.*
        Refresh_Interval  5

    [INPUT]
        Name              tail
        Path              /var/log/containers/*_dev_*.log
        Parser            docker
        Tag               dev.*
        Refresh_Interval  5

    [FILTER]
        Name                kubernetes
        Match               *
        Kube_URL            https://kubernetes.default.svc:443
        Kube_Tag_Prefix     kube.var.log.containers.
        Merge_Log           On
        Keep_Log            Off

    [OUTPUT]
        Name                cloudwatch_logs
        Match               prod.*
        region              us-east-1
        log_group_name      /aws/eks/invokeai/prod
        auto_create_group   true

    [OUTPUT]
        Name                cloudwatch_logs
        Match               dev.*
        region              us-east-1
        log_group_name      /aws/eks/invokeai/dev
        auto_create_group   true
```

### 9.4 Grafana ëŒ€ì‹œë³´ë“œ

**InvokeAI ì»¤ìŠ¤í…€ ëŒ€ì‹œë³´ë“œ:**

```json
{
  "dashboard": {
    "title": "InvokeAI Production Dashboard",
    "panels": [
      {
        "title": "API Server CPU",
        "targets": [
          {
            "expr": "rate(container_cpu_usage_seconds_total{namespace=\"prod\",pod=~\"api-server-.*\"}[5m])"
          }
        ]
      },
      {
        "title": "GPU Worker Status",
        "targets": [
          {
            "expr": "up{job=\"gpu-worker\",namespace=\"prod\"}"
          }
        ]
      },
      {
        "title": "Queue Length",
        "targets": [
          {
            "expr": "redis_queue_length{namespace=\"prod\"}"
          }
        ]
      },
      {
        "title": "Image Generation Rate",
        "targets": [
          {
            "expr": "rate(image_generation_total{namespace=\"prod\"}[5m])"
          }
        ]
      }
    ]
  }
}
```

---

## 10. CI/CD íŒŒì´í”„ë¼ì¸

### 10.1 GitHub Actions (ì „ì²´ íŒŒì´í”„ë¼ì¸)

```yaml
# .github/workflows/deploy.yml
name: Deploy to EKS

on:
  push:
    branches:
      - main  # ìš´ì˜ ë°°í¬
      - develop  # ê°œë°œ ë°°í¬

env:
  AWS_REGION: us-east-1
  ECR_REGISTRY: 123456789.dkr.ecr.us-east-1.amazonaws.com
  EKS_CLUSTER_NAME: invokeai-cluster

jobs:
  # 1. í…ŒìŠ¤íŠ¸
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-dev.txt

      - name: Run tests
        run: |
          pytest tests/ -v --cov=invokeai

      - name: Lint
        run: |
          ruff check .
          black --check .

  # 2. ë¹Œë“œ ë° í‘¸ì‹œ
  build:
    needs: test
    runs-on: ubuntu-latest
    outputs:
      image_tag: ${{ steps.meta.outputs.tags }}
    steps:
      - uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v1

      - name: Docker meta
        id: meta
        uses: docker/metadata-action@v4
        with:
          images: ${{ env.ECR_REGISTRY }}/invokeai-api
          tags: |
            type=ref,event=branch
            type=sha,prefix={{branch}}-

      - name: Build and push
        uses: docker/build-push-action@v4
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  # 3. ë°°í¬ (ìš´ì˜)
  deploy-prod:
    needs: build
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    environment:
      name: production
      url: https://api.invokeai.com
    steps:
      - uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }}

      - name: Deploy to prod
        run: |
          # ì´ë¯¸ì§€ íƒœê·¸ ì—…ë°ì´íŠ¸
          kubectl set image deployment/api-server \
            api=${{ needs.build.outputs.image_tag }} \
            -n prod

          # ë¡¤ì•„ì›ƒ í™•ì¸
          kubectl rollout status deployment/api-server -n prod --timeout=10m

      - name: Run smoke tests
        run: |
          # Health check
          kubectl run smoke-test --image=curlimages/curl --rm -it --restart=Never -- \
            curl -f http://api-service.prod.svc.cluster.local/api/v1/health

  # 4. ë°°í¬ (ê°œë°œ)
  deploy-dev:
    needs: build
    if: github.ref == 'refs/heads/develop'
    runs-on: ubuntu-latest
    environment:
      name: development
      url: https://dev-api.invokeai.com
    steps:
      - uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }}

      - name: Deploy to dev
        run: |
          kubectl set image deployment/api-server \
            api=${{ needs.build.outputs.image_tag }} \
            -n dev

          kubectl rollout status deployment/api-server -n dev --timeout=5m
```

### 10.2 DB ë§ˆì´ê·¸ë ˆì´ì…˜ (ë³„ë„ Job)

```yaml
# .github/workflows/db-migrate.yml
name: Database Migration

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment'
        required: true
        type: choice
        options:
          - dev
          - prod

jobs:
  migrate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --name invokeai-cluster --region us-east-1

      - name: Run migration
        run: |
          kubectl run db-migrate-${{ github.run_id }} \
            --image=123456789.dkr.ecr.us-east-1.amazonaws.com/invokeai-api:latest \
            --restart=Never \
            --namespace=${{ inputs.environment }} \
            --env-from=configmap/invokeai-config \
            --env-from=secret/db-credentials \
            --command -- alembic upgrade head

          # ë¡œê·¸ í™•ì¸
          kubectl logs -f db-migrate-${{ github.run_id }} -n ${{ inputs.environment }}

          # ì •ë¦¬
          kubectl delete pod db-migrate-${{ github.run_id }} -n ${{ inputs.environment }}
```

### 10.3 Rollback ìŠ¤í¬ë¦½íŠ¸

```bash
# scripts/rollback.sh
#!/bin/bash

NAMESPACE=${1:-prod}
DEPLOYMENT=${2:-api-server}

echo "Rolling back $DEPLOYMENT in $NAMESPACE namespace..."

# ì´ì „ ë²„ì „ìœ¼ë¡œ ë¡¤ë°±
kubectl rollout undo deployment/$DEPLOYMENT -n $NAMESPACE

# ë¡¤ë°± í™•ì¸
kubectl rollout status deployment/$DEPLOYMENT -n $NAMESPACE

echo "Rollback completed!"

# í˜„ì¬ ìƒíƒœ í™•ì¸
kubectl get pods -n $NAMESPACE -l app=$DEPLOYMENT
```

---

## 11. ë¹„ìš© ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸

### 11.1 ì»´í“¨íŒ…

- [ ] GPU ì›Œì»¤ëŠ” Spot ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©
- [ ] Karpenterë¡œ ë¹ ë¥¸ ìŠ¤ì¼€ì¼ë§ (ë…¸ë“œ ë‚­ë¹„ ìµœì†Œí™”)
- [ ] HPAë¡œ Pod ìë™ ìŠ¤ì¼€ì¼ë§
- [ ] ì•¼ê°„/ì£¼ë§ ìµœì†Œ ë ˆí”Œë¦¬ì¹´ ì„¤ì •
- [ ] Reserved Instances (1ë…„ ì•½ì • ì‹œ 40% í• ì¸)

### 11.2 ìŠ¤í† ë¦¬ì§€

- [ ] EFS Infrequent Access (30ì¼ í›„ ìë™ ì´ë™)
- [ ] S3 Lifecycle Policy (90ì¼ í›„ Glacier)
- [ ] EBS GP3 ì‚¬ìš© (GP2ë³´ë‹¤ 20% ì €ë ´)
- [ ] ë¶ˆí•„ìš”í•œ ìŠ¤ëƒ…ìƒ· ì‚­ì œ

### 11.3 ë„¤íŠ¸ì›Œí¬

- [ ] CloudFrontë¡œ S3 ìš”ì²­ ê°ì†Œ
- [ ] NAT Gateway ìµœì†Œí™” (VPC Endpoint ì‚¬ìš©)
- [ ] ë°ì´í„° ì „ì†¡ ìµœì í™” (ì••ì¶•)

### 11.4 ë°ì´í„°ë² ì´ìŠ¤

- [ ] Aurora Serverless v2 (ì‚¬ìš©ëŸ‰ì— ë”°ë¼ ìë™ ìŠ¤ì¼€ì¼)
- [ ] Read Replicaë¡œ ì½ê¸° ë¶„ì‚°
- [ ] Connection Pooling (PgBouncer)

---

## 12. ë³´ì•ˆ ì²´í¬ë¦¬ìŠ¤íŠ¸

### 12.1 ë„¤íŠ¸ì›Œí¬

- [ ] Private Subnetì— Worker ë°°ì¹˜
- [ ] Security Group ìµœì†Œ ê¶Œí•œ
- [ ] NetworkPolicyë¡œ Pod ê°„ ê²©ë¦¬
- [ ] WAF ì„¤ì • (SQL Injection, XSS ë°©ì–´)

### 12.2 ì¸ì¦/ì¸ê°€

- [ ] IRSA (IAM Roles for Service Accounts)
- [ ] Secrets Managerë¡œ ë¯¼ê° ì •ë³´ ê´€ë¦¬
- [ ] Podì— IAM ì—­í•  ì§ì ‘ í• ë‹¹ ê¸ˆì§€

### 12.3 ë°ì´í„°

- [ ] Aurora ì•”í˜¸í™” (KMS)
- [ ] EBS ì•”í˜¸í™”
- [ ] S3 ë²„í‚· ì•”í˜¸í™”
- [ ] TLS/SSL í†µì‹ 

---

## 13. íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 13.1 Podê°€ Pending ìƒíƒœ

```bash
# ì´ë²¤íŠ¸ í™•ì¸
kubectl describe pod <pod-name> -n <namespace>

# ë…¸ë“œ ë¦¬ì†ŒìŠ¤ í™•ì¸
kubectl top nodes

# í”í•œ ì›ì¸:
# 1. ë¦¬ì†ŒìŠ¤ ë¶€ì¡± â†’ Cluster Autoscaler í™•ì¸
# 2. Taints ë¶ˆì¼ì¹˜ â†’ tolerations í™•ì¸
# 3. PVC ì—°ê²° ì‹¤íŒ¨ â†’ PVC ìƒíƒœ í™•ì¸
```

### 13.2 GPU Podê°€ ìŠ¤ì¼€ì¤„ë§ ì•ˆ ë¨

```bash
# GPU ë…¸ë“œ í™•ì¸
kubectl get nodes -l nvidia.com/gpu=true

# NVIDIA Device Plugin í™•ì¸
kubectl get pods -n kube-system | grep nvidia

# GPU ë¦¬ì†ŒìŠ¤ í™•ì¸
kubectl describe node <gpu-node> | grep nvidia.com/gpu
```

### 13.3 DB ì—°ê²° ì‹¤íŒ¨

```bash
# Secret í™•ì¸
kubectl get secret db-credentials -n prod -o yaml

# DB ì—°ê²° í…ŒìŠ¤íŠ¸
kubectl run psql-test --rm -it --image=postgres:15 --restart=Never -- \
  psql -h <aurora-endpoint> -U invokeai_prod -d invokeai

# Security Group í™•ì¸ (EKS â†’ Aurora)
```

---

## 14. ì°¸ê³  ìë£Œ

### AWS ë¬¸ì„œ

- [EKS Best Practices](https://aws.github.io/aws-eks-best-practices/)
- [Aurora PostgreSQL](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/)
- [EFS Performance](https://docs.aws.amazon.com/efs/latest/ug/performance.html)

### Kubernetes ë¬¸ì„œ

- [HPA Walkthrough](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/)
- [GPU Support](https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/)
- [Network Policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/)

### ë„êµ¬

- [k9s](https://k9scli.io/) - Kubernetes CLI UI
- [Lens](https://k8slens.dev/) - Kubernetes IDE
- [kubectx/kubens](https://github.com/ahmetb/kubectx) - Context/Namespace ì „í™˜

---

**ì‘ì„± ì™„ë£Œ!** ğŸ‰

ì´ ê°€ì´ë“œëŠ” InvokeAIë¥¼ EKS ê¸°ë°˜ êµ¬ë…í˜• SaaSë¡œ ì „í™˜í•˜ëŠ” ì™„ì „í•œ ë¡œë“œë§µì…ë‹ˆë‹¤.

**ë‹¤ìŒ ë‹¨ê³„:**
1. Terraformìœ¼ë¡œ ì¸í”„ë¼ êµ¬ì¶•
2. ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë° Aurora ì„¤ì •
3. ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬
4. ëª¨ë‹ˆí„°ë§ êµ¬ì„±
5. CI/CD íŒŒì´í”„ë¼ì¸ ì„¤ì •

**ì˜ˆìƒ ì†Œìš” ì‹œê°„:** 8-10ì£¼

**í–‰ìš´ì„ ë¹•ë‹ˆë‹¤!** ğŸš€
